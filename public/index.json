[{"content":" Introduction: Navigating the Complex Landscape of International Data Storage In today\u0026rsquo;s age of digital globalization, businesses are no longer confined to geographic borders. As enterprises spread their wings across continents, they encounter not just diverse cultures and markets, but also a myriad of technical challenges. Foremost among these challenges is the question of data storage. How should businesses store, manage, and access the vast amounts of data they generate and utilize, especially when operating in multiple countries or regions?\nData is the lifeblood of modern businesses, driving everything from daily operations to strategic decision-making. But as businesses expand internationally, the sheer volume, variety, and velocity of data grow exponentially. Add to this the intricate web of regional data regulations, user expectations for speedy access, and the operational nuances of different markets, and you have a complex puzzle to solve.\nChoosing the right data storage strategy is not just a technical decision; it\u0026rsquo;s a strategic one. It impacts user experience, operational efficiency, regulatory compliance, and even the bottom line. As such, understanding the various data storage options, their benefits, and their challenges becomes crucial.\nIn this exploration, we\u0026rsquo;ll delve into the most prominent data storage strategies for international businesses, shedding light on their intricacies and offering insights to help businesses make informed decisions. Whether you\u0026rsquo;re a startup looking to venture abroad for the first time or a multinational conglomerate reevaluating your data strategy, this guide aims to equip you with the knowledge to navigate the dynamic landscape of international data storage.\nLocalized Data Storage In the realm of international expansion, one strategy that often comes to the forefront is localized data storage. At its core, this approach entails storing data in the same region or country where a business operates. It sounds straightforward, but like all strategies, it has its nuances.\nWhen we talk about the benefits of localized storage, compliance with regional data regulations often tops the list. Many countries, armed with data sovereignty laws like Europe\u0026rsquo;s General Data Protection Regulation (GDPR) or Asia\u0026rsquo;s Personal Data Protection Act (PDPA), mandate that the personal data of their citizens be stored within national borders. By aligning with these laws, businesses not only steer clear of legal complications but also foster trust among local customers. There\u0026rsquo;s something reassuring about knowing your data isn\u0026rsquo;t traveling halfway across the globe, isn\u0026rsquo;t there?\nSpeaking of data travel, localized storage inherently reduces data latency. Imagine trying to access a file stored on a distant server. The longer the distance, the longer the wait. By storing data closer to where it\u0026rsquo;s used, businesses ensure snappier access times, which is a boon for applications that thrive on real-time or near-real-time data. This speed can translate to tangible benefits, like faster transaction processing in physical stores or more responsive customer service.\nBeyond compliance and speed, localized storage offers another enticing advantage: tailored user experiences. With data stored regionally, businesses can tap into local trends, preferences, and cultural nuances. This granularity paves the way for region-specific marketing campaigns or product recommendations, potentially leading to a more loyal and satisfied customer base.\nYet, every silver lining has its cloud. Localized data storage isn\u0026rsquo;t without challenges. The most apparent one is the cost. Utilizing cloud services with data centers in multiple regions can involve higher service fees. Then there\u0026rsquo;s the complexity of ensuring data redundancy and backups across these centers, not to mention the potential for data inconsistencies. Add to that the varied threat landscapes and security protocols of different regions, and you\u0026rsquo;re looking at a more intricate web to navigate.\nAnother layer of complexity arises from managing operations across these multiple data centers. Different regions can have their own sets of standards and technological ecosystems. Synchronizing these, ensuring compatibility, and maintaining top-notch security can require more hands on deck, sophisticated management tools, or specialized training.\nHowever, with the right approach and tools, businesses can leverage the strengths of localized storage while mitigating its challenges. As international landscapes evolve and data becomes an even more precious commodity, understanding the ins and outs of localized data storage will be crucial for any expanding business.\nCertainly! A summary at the end can serve as a quick reference for readers, providing a concise overview of the main points discussed. Here\u0026rsquo;s how it might look:\nSummary: Localized Data Storage Benefits:\nRegulatory Compliance: Adheres to regional data sovereignty laws, reducing legal risks. Trust Building: Enhances trust among local customers, knowing their data is stored nearby. Reduced Latency: Provides faster data access times, enhancing user experience and operational efficiency. Tailored Experiences: Allows for customization based on local trends and preferences. Challenges:\nInfrastructure Costs: Significant investment required for multiple data centers. Data Management: Ensuring redundancy, backups, and consistency across regions can be complex. Operational Hurdles: Managing varied regional standards, technologies, and security landscapes adds layers of complexity. Centralized Data Storage Switching gears from the region-specific approach of localized storage, we arrive at centralized data storage. The idea here is simple: all data, regardless of where it\u0026rsquo;s generated or used, is stored in a single, central location. This strategy offers its own set of advantages and challenges, some of which stand in stark contrast to the localized approach.\nOne of the most compelling arguments for centralized storage is cost-efficiency. Think about it. Rather than provisioning resources across multiple data centers in different cloud regions, businesses can consolidate their cloud storage into a single region. This consolidation can lead to significant cost savings in infrastructure, staffing, and maintenance. It\u0026rsquo;s akin to having one large warehouse instead of several smaller ones scattered around.\nBeyond cost, centralized storage offers a streamlined approach to data management. With all data housed under one roof, tasks like backups, updates, and maintenance become notably simpler. There\u0026rsquo;s no juggling between different regional databases or wrangling with synchronization issues. Everything is unified, leading to a more cohesive data management strategy.\nFrom a user\u0026rsquo;s perspective, centralized storage can ensure a consistent experience. Whether a user is accessing data from New York, Nairobi, or New Delhi, they\u0026rsquo;re tapping into the same central repository. This uniformity can be a boon for global services or platforms aiming to provide a consistent brand or user experience worldwide.\nHowever, centralized storage isn\u0026rsquo;t without its pitfalls. The most glaring concern is the potential for non-compliance with data sovereignty laws. As we touched upon in the localized storage section, many countries mandate that certain data types, especially personal data, remain within national borders. A centralized approach, especially if the data center is located in a different country, might inadvertently breach these laws.\nThen there\u0026rsquo;s the issue of data latency. Unlike localized storage, where data is a short hop away from the end user, centralized storage can sometimes mean data has to travel vast distances. This can introduce delays, especially for users located far from the central data center.\nLastly, a centralized system introduces a vulnerability: a single point of failure. If something goes awry at the central data center—be it a technical glitch, a natural disaster, or a security breach—it has the potential to disrupt access for all users, regardless of their location. It\u0026rsquo;s the proverbial \u0026ldquo;all eggs in one basket\u0026rdquo; scenario.\nSummary: Centralized Data Storage Benefits:\nCost-Efficiency: Significant savings from maintaining one central data center. Unified Data Management: Streamlined backups, updates, and maintenance. Consistent User Experience: Uniform access and experience for users globally. Challenges:\nRegulatory Risks: Potential non-compliance with data sovereignty laws. Data Latency: Possible delays for users distant from the central location. Single Vulnerability: Risks associated with having a single point of failure. Multi-Cloud Storage Journeying further into the world of data storage strategies, we encounter the concept of multi-cloud storage. Now, if you\u0026rsquo;re imagining a sky filled with multiple clouds each holding a piece of your data, you\u0026rsquo;re not entirely off track. Multi-cloud storage is about diversifying your data storage by leveraging multiple cloud service providers instead of relying solely on one.\nSo, why would a business want to scatter its data across different clouds? First and foremost, it\u0026rsquo;s about not putting all your trust in a single entity. By distributing data across multiple providers, businesses can achieve a level of redundancy. If one cloud provider faces an outage or a security breach, the others can still function, ensuring uninterrupted access to crucial data.\nCost efficiency, though perhaps surprising, is another compelling reason. Different cloud providers have varying pricing models, specialties, and promotional offers. By astutely distributing workloads based on these factors, businesses can optimize costs. Think of it as shopping at different stores for the best deals on each item on your list.\nFurthermore, multi-cloud storage can be a strategic move to tap into the unique strengths of each provider. One cloud service might offer superior analytics tools, while another excels in machine learning capabilities. By diversifying, businesses can harness the best of all worlds.\nHowever, as with all strategies, multi-cloud storage brings its set of challenges. Managing multiple cloud services like AWS and GCP can be complex, as each provider has its own set of APIs, services, and pricing models. This can lead to a steep learning curve and potential operational hiccups.\nSecurity is another area that needs meticulous attention. Ensuring consistent security protocols across different clouds can be daunting. After all, each cloud provider has its own security architecture and policies.\nLastly, data transfer costs between clouds can add up. While storing data in multiple clouds offers flexibility, moving data between them—should the need arise—can incur charges.\nSummary: Multi-Cloud Storage Benefits:\nRedundancy: Reduced risk of data loss or inaccessibility due to reliance on multiple providers. Cost Optimization: Potential savings by leveraging the strengths and pricing models of different providers. Harnessing Strengths: Ability to use the unique features or tools of each cloud service. Challenges:\nOperational Complexity: Managing multiple cloud interfaces and tools. Security Consistency: Ensuring uniform security measures across different clouds. Data Transfer Costs: Potential charges for moving data between cloud providers. Edge Storage (or Edge Computing) As we continue our exploration of data storage strategies, we edge closer to, well, the \u0026ldquo;edge.\u0026rdquo; The term might sound cutting-edge, and in many ways, it is. Edge storage, often intertwined with edge computing, revolves around the idea of processing and storing data closer to where it\u0026rsquo;s generated or consumed. Imagine a bustling cafe in Tokyo processing orders in real time, or a traffic management system in Amsterdam analyzing data on the spot. That\u0026rsquo;s edge computing in action.\nThe most immediate benefit of edge storage is the dramatic reduction in latency. When data doesn\u0026rsquo;t need to travel back and forth to a distant central server but is instead processed on-site or nearby, the response time can be lightning fast. This is a game-changer for applications demanding real-time reactions, like autonomous vehicles or instant payment processing in stores.\nAnother advantage is the potential bandwidth savings. By processing data locally and only sending relevant, often aggregated, information back to the central system, businesses can reduce the amount of data traversing the network. This can lead to significant cost savings, especially in areas where bandwidth is pricey or limited.\nThen there\u0026rsquo;s the element of reliability. Edge storage can operate independently, even if there\u0026rsquo;s a temporary disconnect from the central system. In scenarios where continuous operation is critical, such as healthcare monitoring systems, this autonomy can be lifesaving.\nHowever, the edge approach is not devoid of challenges. One of the primary concerns is security. Ensuring robust security measures on numerous edge devices, which might be more exposed than centralized data centers, can be a daunting task.\nManaging and updating a vast array of edge devices can also be complex. Unlike a central system where updates can be rolled out uniformly, edge devices might require individual attention.\nMoreover, while edge storage provides autonomy, it also necessitates efficient synchronization with the central system. Ensuring data consistency and timely updates across numerous edge locations can be a meticulous task.\nSummary: Edge Storage (or Edge Computing) Benefits:\nReduced Latency: Near-instantaneous data processing for real-time applications. Bandwidth Savings: Less data sent over the network, leading to reduced costs. Operational Reliability: Continues functioning even if disconnected from the central system. Challenges:\nSecurity Concerns: Ensuring robust security on multiple, potentially exposed, edge devices. Device Management: Overseeing updates and maintenance on numerous devices. Data Synchronization: Maintaining consistency and timely updates with the central system. An Integrated Approach: Central + Local + Edge In the vast tapestry of data storage strategies, a new paradigm is emerging, one that harmoniously blends centralized, localized, and edge storage. This integrated approach seeks to offer the best of all worlds, aiming to navigate the complexities of international business with finesse.\nImagine a vast network where each component has a distinct role, yet they all work in concert.\nLocalized Storage often provisioned through regional cloud services, is like the regional hubs, closely attuned to the local pulse. Here, all data specific to a region—be it PII (Personal Identifiable Information) or general data—is securely stored in the cloud. This not only ensures adherence to regional data regulations but also guarantees swift access for operations within that region. It\u0026rsquo;s as if each region has its own data vault, fortified and efficient.\nCentralized Storage often hosted on a cloud service, acts as the grand repository, the epicenter of this network. But instead of hoarding all data, it focuses on cleaned non-PII data aggregated from all regions. Picture a central command center in the US, continuously updated with global data, driving analytics, insights, and overarching decision-making. It\u0026rsquo;s the strategic vantage point, providing a holistic view of operations worldwide.\nAnd then there\u0026rsquo;s the frontline, managed by Edge Storage. These are the real-time nerve centers, be it a store in Paris or an outlet in Sydney. Here, operations are instantaneous—processing orders, updating inventories, and handling immediate tasks. But they aren\u0026rsquo;t isolated. They\u0026rsquo;re set up with pipelines, ensuring relevant data flows seamlessly to either the local hubs or the central repository as needed.\nThe allure of this approach is undeniable. It promises a seamless integration that caters to global business needs while being deeply rooted in local compliance and operational immediacy.\nHowever, with great integration comes great responsibility. Orchestrating the flow of data across this intricate network is no small feat. Ensuring data consistency, especially with real-time operations at the edge, demands vigilant oversight. Then there\u0026rsquo;s the potential latency to consider, especially between the edge and the central storage.\nSummary: An Integrated Approach Benefits:\nHolistic Strategy: Combines the strengths of central, local, and edge storage. Operational Efficiency: Ensures both global insights and real-time regional responses. Regulatory Compliance: Localized storage ensures adherence to regional data laws. Challenges:\nData Flow Management: Orchestrating seamless data movement across the network. Consistency: Ensuring uniform data across the entire system. Latency Concerns: Potential delays, especially between edge and central storage. Addressing Latency in the Integrated Approach Latency, often dubbed the silent killer in the realm of data-driven operations, is a pivotal concern, especially when juggling a blend of centralized, localized, and edge storage strategies. In an integrated approach, ensuring that data is both timely and relevant is paramount. So, how can businesses minimize latency while ensuring critical systems operate close to real time?\n1. Data Caching: By strategically caching frequently accessed data at the edge or local storage, businesses can dramatically reduce retrieval times. This ensures that users and applications can access this data without the delay of reaching back to the central storage. Think of it as having a mini-library of the most popular books right in your neighborhood.\n2. Efficient Data Pipelines: Designing efficient data pipelines that prioritize the flow of time-sensitive data can be a game-changer. This might involve setting up dedicated channels or utilizing high-speed data transfer protocols to move data swiftly between edge, local, and central storages.\n3. Predictive Analytics: By leveraging analytics, businesses can predict which data points are likely to be accessed frequently or are of high importance. This foresight allows for preemptive data positioning, ensuring that data is already where it\u0026rsquo;s most likely to be needed, reducing access times.\n4. Load Balancing: Distributing data requests across multiple servers or nodes can prevent any single point from becoming a bottleneck. Load balancers can intelligently redirect requests to ensure optimal performance and reduce wait times.\n5. Real-time Data Compression: Before transmitting data, especially over long distances, compressing it can reduce the volume of data that needs to travel. While this involves a trade-off—compression and decompression times—it can often result in net time savings for large datasets.\n6. Periodic Synchronization: Instead of continuous data synchronization, which can introduce delays, opting for periodic syncs during off-peak hours can ensure data consistency without hampering real-time operations.\nBy meticulously implementing these strategies and continuously monitoring performance, businesses can ensure that their integrated approach remains nimble and responsive. While latency can never be entirely eliminated, with the right tools and strategies, it can be tamed to an extent where its impact on real-time operations is negligible.\nSummary: Addressing Latency Strategies:\nData Caching: Store frequently accessed data closer to users. Efficient Data Pipelines: Prioritize and speed up the flow of time-sensitive data. Predictive Analytics: Anticipate data needs and position data accordingly. Load Balancing: Distribute data requests to prevent bottlenecks. Real-time Data Compression: Reduce data volume during transfers. Periodic Synchronization: Sync data during off-peak hours for consistency without delays. Conclusion: Crafting the Perfect Data Storage Symphony In our expedition across various data storage strategies, we\u0026rsquo;ve unraveled the intricacies of each, from the region-specific nuances of localized storage to the intricate ballet of an integrated approach. Data, often dubbed as the \u0026rsquo;new oil,\u0026rsquo; is pivotal in driving business decisions, especially in an era of rampant globalization. Hence, understanding and optimizing its storage becomes paramount.\nThe international expansion isn\u0026rsquo;t just about establishing a physical presence in new territories. It\u0026rsquo;s about embedding a brand into new cultures, markets, and ecosystems. And for that, data plays a vital role. Whether it\u0026rsquo;s understanding local buying habits or ensuring seamless real-time operations, the backbone remains the same: a robust, efficient, and compliant data storage strategy.\nAn integrated approach, as we\u0026rsquo;ve explored, can be the golden mean for businesses looking to expand internationally. By amalgamating the strengths of centralized, localized, and edge storage, it offers a framework that\u0026rsquo;s both globally insightful and locally attuned. However, no one-size-fits-all. The optimal strategy for any business hinges on its specific goals, challenges, and the regions it targets.\nAdaptability remains the cornerstone of successful international expansion. Just as businesses adapt their products, services, or marketing strategies to new markets, so must they adapt their data strategies. By staying agile and leveraging the strengths of various storage methodologies, businesses can not only ensure smooth operations but also glean invaluable insights that can drive growth.\nIn the grand tapestry of international business, where each market presents its own set of challenges and opportunities, having a flexible and efficient data strategy is like having a trustworthy compass. It guides, informs, and ensures that every decision, big or small, is backed by the power of data.\nTo sum it up, as businesses stride forward in their global endeavors, the right data storage strategy can be the wind beneath their wings, propelling them to new heights and horizons.\nFor those who appreciate a comparative glance, here\u0026rsquo;s a brief comparison of the different strategies we discussed:\nLocalized Storage: Region-specific, ensures compliance, ideal for businesses deeply rooted in local markets. Centralized Storage: Cost-efficient, offers a unified data experience, best for brands seeking a consistent global image. Multi-Cloud Storage: Redundant, taps into the strengths of various cloud providers, perfect for businesses seeking flexibility and reduced vendor lock-in. Edge Storage: Real-time, processes data at the source, optimal for operations demanding instantaneous data access. Integrated Approach: The best of all worlds, caters to both global and local needs, ideal for businesses looking for a balanced, holistic strategy. In the end, the choice of strategy rests on a business\u0026rsquo;s unique needs, challenges, and aspirations. And as the business landscape evolves, so too will the strategies. Staying informed, adaptable, and forward-looking is the key.\n","permalink":"https://blog.oll.is/posts/2023-09-22_international_storage_options/","summary":"This guide delves into various data storage strategies that are crucial for international businesses. It covers the benefits and challenges of localized, centralized, multi-cloud, and edge storage, along with an integrated approach that combines these strategies. The post aims to equip businesses with the insights needed to make informed data storage decisions.","title":"International Data Storage: Strategies and Challenges for Global Businesses"},{"content":" Introduction The Asia-Pacific (APAC) region is a diverse landscape of data protection laws, making it a complex area for data architects to navigate. This post aims to delve deeper into specific regulations within the APAC region, focusing on a range of countries from Singapore to the Philippines. As in the original post, we\u0026rsquo;ll contrast these against the California Consumer Privacy Act (CCPA) to provide a relatable point of reference for U.S.-based professionals. For a recap on the CCPA, please see the first blog post in this series.\nRegulations Singapore: Personal Data Protection Act (PDPA) Intro The PDPA is Singapore\u0026rsquo;s primary data protection law, governing the collection, use, and disclosure of personal data. It aims to safeguard individuals\u0026rsquo; personal data against misuse and ensure organizational compliance.\nImplementation To comply with PDPA, data architects should implement explicit consent mechanisms and robust data protection policies, including data encryption and user authentication features.\nSimilar to CCPA Both PDPA and CCPA emphasize consumer rights and require transparency in data collection practices. Different from CCPA PDPA mandates the appointment of a Data Protection Officer (DPO), which is not required under CCPA. Key Points for Data Architects Consent and Notification: Implement explicit consent mechanisms. Data Localization: Unlike CCPA, PDPA has specific data localization requirements. Data Protection Officer: Appointment of a DPO is mandatory. Japan: Act on the Protection of Personal Information (APPI) Intro The APPI is Japan\u0026rsquo;s foundational data protection regulation, focusing on the protection of individuals\u0026rsquo; rights and personal data.\nImplementation Data architects should be aware of the \u0026ldquo;Opt-in\u0026rdquo; consent model for sensitive information and the \u0026ldquo;Opt-out\u0026rdquo; model for non-sensitive information, which are central to APPI compliance.\nSimilar to CCPA Both APPI and CCPA require businesses to disclose their data collection practices to consumers. Different from CCPA APPI has a broader definition of what constitutes \u0026lsquo;personal information\u0026rsquo;. Key Points for Data Architects Explicit and Implicit Consent: Be aware of the different consent models for sensitive and non-sensitive information. Data Protection Management: Businesses are required to take necessary and appropriate action to secure personal data. Australia: Privacy Act Intro Australia\u0026rsquo;s Privacy Act governs the handling of personal information and is aimed at protecting the privacy of individuals.\nImplementation Data architects should focus on the 13 Australian Privacy Principles (APPs) which outline how personal information should be handled, used, and processed.\nSimilar to CCPA Both the Privacy Act and CCPA offer individuals the right to access and correct their data. Different from CCPA The Privacy Act includes specific regulations for the use of personal data for direct marketing, which CCPA does not explicitly cover. Key Points for Data Architects Direct Marketing: Be aware of the specific guidelines for using personal data in marketing. Data Breach Notification: A Notifiable Data Breaches (NDB) scheme is in place, requiring organizations to notify affected individuals and the Australian Information Commissioner of serious data breaches. China: Personal Information Protection Law (PIPL) Intro China\u0026rsquo;s PIPL, which came into effect in 2021, is considered China\u0026rsquo;s first comprehensive data protection law. It aims to regulate the processing of personal information and protect individual rights.\nImplementation Data architects should be aware that PIPL emphasizes individual consent, data minimization, and enhanced data security measures.\nSimilar to CCPA Both PIPL and CCPA focus on consumer rights, such as the right to access and delete personal data. Different from CCPA PIPL introduces the concept of \u0026ldquo;important data,\u0026rdquo; which involves stricter regulations and potential scrutiny from Chinese authorities. Key Points for Data Architects Important Data: Understand and classify what falls under \u0026ldquo;important data\u0026rdquo; and the corresponding regulations. Data Localization: PIPL has stringent data localization requirements, including storing data within China\u0026rsquo;s borders. Consent Mechanisms: Explicit consent is often required, similar to a CCPA-compliant pop-up for user opt-in or opt-out. South Korea: Personal Information Protection Act (PIPA) Intro South Korea\u0026rsquo;s PIPA is its primary legislation for data protection, focusing on the collection, use, and dissemination of personal data.\nImplementation Data architects should focus on implementing robust consent mechanisms and encryption methods to protect personal data.\nSimilar to CCPA Both PIPA and CCPA require a transparent privacy policy. Different from CCPA PIPA has stricter penalties for non-compliance, including imprisonment. Key Points for Data Architects Encryption: Strong encryption is often required to protect personal data. User Consent: Detailed consent forms may be necessary for data collection. Penalties: Be aware of the severe penalties for non-compliance, including potential imprisonment. India: Personal Data Protection Bill (PDPB) Intro India\u0026rsquo;s PDPB recently passed in August 2023 and aims to safeguard the privacy of individuals\u0026rsquo; personal data.\nImplementation Data architects should prepare for explicit user consent requirements and data localization stipulations.\nSimilar to CCPA Both PDPB and CCPA focus on user consent for data collection and processing. Different from CCPA PDPB may require the storage of a copy of all personal data on servers located within India. Key Points for Data Architects Data Localization: Prepare for potential data localization requirements. User Consent: Explicit consent mechanisms will be crucial for compliance. Data Processing Limitations: Limited data processing based on the purpose is likely to be a key requirement. Malaysia: Personal Data Protection Act (PDPA) Intro Malaysia\u0026rsquo;s PDPA aims to regulate the processing of personal data in commercial transactions, providing for the rights of individuals to have their personal data secured.\nImplementation Data architects should focus on lawful and fair data processing methods, along with explicit consent mechanisms.\nSimilar to CCPA Both PDPA and CCPA require organizations to inform consumers about how their data will be used. Different from CCPA PDPA includes specific requirements for data integrity, which CCPA does not explicitly cover. Key Points for Data Architects Data Integrity: Ensuring the accuracy and completeness of data is crucial. User Consent: Explicit consent for data collection and processing is often required. Data Security: Robust security measures are essential to protect personal data. Indonesia: Minister of Communication and Informatics Regulation (MOCI) Intro Indonesia\u0026rsquo;s MOCI Regulation serves as the country\u0026rsquo;s primary data protection framework, focusing on the electronic systems operators\u0026rsquo; obligations to manage personal data properly.\nImplementation Data architects should implement robust data protection measures, including encryption, and focus on acquiring explicit user consent.\nSimilar to CCPA Both MOCI and CCPA require businesses to disclose their data collection practices to consumers. Different from CCPA MOCI mandates the need for a Data Protection Officer (DPO), which is not explicitly required under CCPA. Key Points for Data Architects Data Protection Officer: Appointing a DPO is often required. User Consent: Explicit consent for data collection and processing is a necessity. Data Security: Implement robust security measures, including encryption. Philippines: Data Privacy Act (DPA) Intro The Philippines\u0026rsquo; DPA focuses on protecting personal information processed by both public and private organizations.\nImplementation Data architects should prioritize obtaining informed consent from data subjects and ensuring the secure storage and disposal of personal information.\nSimilar to CCPA Both the DPA and CCPA provide consumers with the right to access and correct their data. Different from CCPA DPA requires organizations to appoint a Data Protection Officer (DPO), which is not a requirement under CCPA. Key Points for Data Architects Data Protection Officer: The appointment of a DPO is mandatory. User Consent: Informed consent is required for data collection. Data Security: Implement secure storage and disposal mechanisms. The Role of a Data Protection Officer (DPO) in APAC Regulations What is a Data Protection Officer? A Data Protection Officer (DPO) is an individual appointed within an organization to oversee data protection activities. The DPO acts as an internal auditor and advisor, ensuring that an organization\u0026rsquo;s data handling practices are compliant with relevant data protection laws. The role is particularly crucial in regions like the APAC, where several countries mandate the appointment of a DPO.\nResponsibilities of a DPO The responsibilities of a DPO typically include:\nAdvising on Compliance: The DPO advises the organization on how to comply with data protection laws and regulations.\nTraining and Awareness: The DPO conducts training sessions and creates awareness among staff about data protection measures.\nMonitoring and Auditing: Regularly reviewing data protection measures and policies to ensure they are effective and compliant with laws.\nData Breach Response: In the event of a data breach, the DPO is responsible for notifying regulatory authorities and affected individuals, as required by law.\nImportance in the APAC Region In the APAC region, countries like Singapore (PDPA), Indonesia (MOCI), and the Philippines (DPA) mandate the appointment of a DPO. Even in countries where a DPO is not explicitly required, having one can help organizations navigate the complex landscape of APAC data protection regulations effectively.\nSimilarities and Differences with CCPA Similar to CCPA While CCPA does not mandate a DPO, it does require organizations to provide a method for consumers to exercise their privacy rights, a role often fulfilled by a DPO in practice. Different from CCPA Unlike many APAC regulations, CCPA does not explicitly require the appointment of a DPO, making it less stringent in this aspect. Key Points for Data Architects Understanding DPO Requirements: Know whether the countries you operate in require a DPO and what qualifications they must have.\nConsultation: In systems design and data architecture planning, consultation with the DPO can help ensure compliance and effective risk management.\nConclusion Understanding the intricacies of APAC data regulations is crucial for data architects, especially for those looking to expand their organizational reach within this diverse region. This post aims to serve as a comprehensive guide, contrasting APAC laws against the widely-adopted CCPA to provide valuable insights into data compliance and effective architecture design in the APAC region.\nImportant Things to Remember China\u0026rsquo;s PIPL: Unlike the CCPA, China\u0026rsquo;s PIPL introduces the concept of \u0026ldquo;important data,\u0026rdquo; which can be subject to extra scrutiny. The law also has stringent data localization requirements, making it notably more restrictive compared to U.S. regulations. South Korea\u0026rsquo;s PIPA: This law includes severe penalties for non-compliance, including potential imprisonment. This makes it crucial for data architects to be exceptionally diligent when working in or with South Korea. India\u0026rsquo;s PDPB: Although still under legislative review, the proposed Personal Data Protection Bill in India may require the storage of a copy of all personal data on servers located within India, which could be a logistical challenge for many organizations. Including a Data Protection Officer (DPO) in your data strategy can help navigate these complexities, especially in countries where a DPO is mandated. Whether you\u0026rsquo;re new to APAC regulations or looking to refine your existing data protection strategies, always consult legal experts to ensure you\u0026rsquo;re on the right side of the law.\nSummary Comparison Table of APAC Data Regulations Regulation (Country) Consumer Rights Explicit Consent Data Localization Data Breach Reporting Penalties Data Subject Rights Cross-border Data Transfer Regulatory Oversight PDPA (Singapore) Yes Yes Yes Yes Moderate Yes Restricted Personal Data Protection Commission APPI (Japan) Yes Yes No Yes Moderate Yes Yes PPC Privacy Act (Australia) Yes Yes No Yes Moderate Yes Restricted OAIC PIPL (China) Yes Yes Yes Yes High Yes Restricted Cyberspace Administration of China PIPA (South Korea) Yes Yes No Yes High Yes Yes PIPC PDPB (India) Yes Yes Yes Yes Moderate Yes Restricted DPAI PDPA (Malaysia) Yes Yes No Yes Moderate Yes Yes PDP Commission MOCI (Indonesia) Yes Yes No Yes Moderate Yes Yes Ministry of Communication and Informatics DPA (Philippines) Yes Yes No Yes Moderate Yes Yes National Privacy Commission ","permalink":"https://blog.oll.is/posts/2023-09-15_apac_data_regulations/","summary":"A high level summary of Data Regulations including Japan, Singapore, Australia, China, South Korea, India, Malaysia, Indonesia, and the Philippines","title":"In-depth Comparison of APAC International Data Regulations"},{"content":" Introduction Why Robust Code Matters In the realm of software development, robust code serves as the bedrock of reliable and efficient systems. It\u0026rsquo;s more than just writing code that \u0026ldquo;works\u0026rdquo;; it\u0026rsquo;s about crafting code that can withstand a range of challenges including unexpected inputs, system changes, and long-term scaling. Robust code is integral for long-term maintainability, reducing technical debt, and creating a codebase that is not only functional but also resilient to bugs and errors. In a language as versatile as Python, which finds applications across various domains, the importance of robust code becomes even more pronounced.\nThe Importance of Discipline in Python Python\u0026rsquo;s flexibility and ease of use make it accessible for a wide range of applications, from web development to machine learning to automation. However, this very versatility can be a double-edged sword. The internet is rife with Python code examples that, while functional, may not follow best practices. For those new to Python or coming from different domains, these suboptimal examples can set a misleading precedent. That\u0026rsquo;s why discipline is paramount. Being disciplined means adhering to best practices even when Python\u0026rsquo;s flexibility allows you to bypass them. It\u0026rsquo;s about going the extra mile to make your code not just functional, but robust, maintainable, and a pleasure to work with. This disciplined approach is foundational to all the robust coding practices that follow.\nWhat Is \u0026lsquo;Robust Code\u0026rsquo; In Python, robust code is a combination of resilience, maintainability, and readability. Resilient code can gracefully handle unexpected situations; maintainable code is structured to allow easy updates and debugging; readable code is self-explanatory and easy for other developers to understand. When you achieve these three pillars, you create a robust codebase that serves as a strong foundation for any project, big or small.\nCode Organization Modularization In Python, modular code is not just a best practice—it\u0026rsquo;s a sanity-saving necessity, especially for larger projects. Modularization involves breaking down your code into separate functions, modules, and packages, each responsible for a specific piece of functionality. This makes your code easier to test, debug, and extend. For example, a function that calculates the area of a circle should only do just that, making it reusable and easier to test.\n# Good Practice def calculate_area(radius): return 3.14159 * radius * radius Naming Conventions Good naming conventions are like good hygiene for your code; they make it easier to read, understand, and collaborate on. Python has a set of naming conventions outlined in PEP 8, which most Pythonistas try to follow.\nFor classes, use CamelCase:\nGood: class UserProfile: Bad: class user_profile: For functions and parameters, use snake_case:\nGood: def calculate_area(radius): Bad: def CalculateArea(Radius): For private methods and private instance variables, prepend a single underscore:\nGood: _private_method(), _private_variable Bad: privatemethod(), privateVariable By adhering to these naming conventions, you make your code more accessible and understandable to other Python developers, thereby contributing to its robustness.\nMethod Length: Keep It Short and Sweet One of the simplest yet most impactful measures of code organization is the length of your methods or functions. A common best practice is to keep method lengths between 20 to 30 lines. This isn\u0026rsquo;t a strict rule, but rather a guideline to encourage developers to write methods that do one thing and do it well.\nShorter methods have several advantages:\nReadability: They are easier to read and understand. When a method is concise, its purpose is usually clear. Maintainability: Smaller, focused methods are easier to modify without introducing bugs. Reusability: Short methods that perform a specific function are more likely to be reusable in different parts of the codebase. Testability: Methods that stick to a single responsibility are simpler to test, leading to more robust code. It\u0026rsquo;s worth noting that, sometimes, a method may need to exceed this guideline, especially if making it shorter would compromise clarity. However, if you regularly find yourself writing lengthy methods, it might be an indicator to reconsider how you\u0026rsquo;re structuring your code.\n# Good Practice def add_numbers(a, b): return a + b # Potential Red Flag def process_data(data): # ... 50 lines of data processing ... In the example above, the add_numbers function is concise and has a clear purpose, while the process_data function might be doing too much and could potentially be broken down into smaller, more focused methods.\nError Handling Python\u0026rsquo;s Error-Prone Behavior Before diving into best practices for error handling, it\u0026rsquo;s crucial to understand that Python can be error-prone in ways that might differ from other programming languages. For example, when searching for an item in an array, languages like C++ or JavaScript return a value like -1 to indicate the item is not found. However, Python throws a ValueError exception in such cases. This behavior necessitates a more proactive approach to error handling.\n// C++ returns -1 when an item is not found in an array // C++ Code std::vector\u0026lt;int\u0026gt; v = {1, 2, 3}; auto it = std::find(v.begin(), v.end(), 4); if (it != v.end()) { // Found } else { // Not found, it would be v.end() } # Python throws an error when an item is not found in a list # Python Code arr = [1, 2, 3] try: index = arr.index(4) except ValueError: print(\u0026#34;Item not found.\u0026#34;) Being aware of these idiosyncrasies is the first step in writing robust Python code that can handle and recover from errors gracefully.\nException Handling One of the hallmarks of robust code is its ability to anticipate and gracefully handle errors. In Python, this is typically done through exception handling using the try, except, finally blocks. Effective exception handling prevents your program from crashing when it encounters an error, allowing it to either recover or exit gracefully, providing useful information about what went wrong. This is invaluable for debugging and for providing meaningful feedback to users.\n# Example of Exception Handling try: result = 10 / 0 except ZeroDivisionError: print(\u0026#34;Caught a division by zero error.\u0026#34;) finally: print(\u0026#34;Cleanup code, if any, goes here.\u0026#34;) Logging Logging is another critical component of robust code. While exceptions handle the \u0026ldquo;exceptional\u0026rdquo; cases, logging helps you understand the regular flow and the anomalies in your program. Effective logging can help you debug errors, understand program flow, and even gather usage statistics. Python’s built-in logging library is a versatile tool for various logging levels (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n# Example of Logging import logging logging.basicConfig(level=logging.INFO) def divide(a, b): try: result = a / b except ZeroDivisionError: logging.error(\u0026#34;Division by zero!\u0026#34;) return None return result Using Types for Robustness Static Typing Python, traditionally a dynamically typed language, introduced optional static typing in Python 3.5 via PEP 484. While the Python runtime itself doesn\u0026rsquo;t enforce these type hints, they serve multiple purposes that contribute to writing robust code. First, type hints make your code self-documenting. Anyone reading the code can immediately understand what types of arguments a function expects and what it returns, making the code more maintainable and less prone to bugs. Second, type hints enable better IDE support. Features like auto-completion, linting, and error checking become more accurate when the IDE knows the expected types. Last but not least, type hints allow for early detection of bugs. Tools like mypy can be used to statically analyze your code, catching potential type errors before the code is even run.\n# Example of Static Typing from typing import List def sum_elements(elements: List[int]) -\u0026gt; int: return sum(elements) # Using mypy for static type checking # Command: mypy your_script.py Efficiency in Iteration List Comprehensions Python’s list comprehensions are not just syntactic sugar; they are a more efficient way to perform operations on lists (or any iterable), compared to traditional for loops. The speed advantage comes from the internal optimizations that Python performs, reducing the overhead of function calls and lookups in the global namespace. This might not be noticeable in smaller lists, but for larger datasets, the difference can be significant.\nList comprehensions are also more readable and concise, allowing you to express complex transformations in a single line of code. However, it\u0026rsquo;s important to strike a balance. Overly complex list comprehensions can become hard to read and maintain.\n# Using a for loop squares = [] for x in range(10): squares.append(x * x) # Using List Comprehension squares = [x * x for x in range(10)] # Complex example (not recommended for readability) result = [(x, y) for x in range(3) for y in range(3) if x != y] By opting for list comprehensions where appropriate, you not only make your code more efficient but also contribute to its readability and maintainability, thereby enhancing its robustness.\nDictionary Comprehensions Just like list comprehensions, Python also offers dictionary comprehensions to create dictionaries in a more efficient and readable manner. Dictionary comprehensions are a way to transform one dictionary into another dictionary, filtering keys, and values and applying a function to the keys and values. They offer the same benefits as list comprehensions: internal optimizations for better performance and concise, readable code.\nUsing dictionary comprehensions, you can transform complex loops that involve dictionary keys and values into a single line of code. However, as with list comprehensions, keeping them simple is essential for readability and maintainability.\n# Using a for loop to create a dictionary squares_dict = {} for x in range(1, 6): squares_dict[x] = x * x # Using Dictionary Comprehension squares_dict = {x: x * x for x in range(1, 6)} # Complex example (not recommended for readability) result_dict = {x: \u0026#39;even\u0026#39; if x % 2 == 0 else \u0026#39;odd\u0026#39; for x in range(1, 6)} By employing list and dictionary comprehensions where they make sense, you contribute to the efficiency, readability, and robustness of your Python code.\nProtocols and Class Signatures Defining Protocols and Duck Typing In Python, the concept of protocols plays a significant role in writing code that is both robust and maintainable. At its core, a protocol specifies a contract that classes should adhere to, much like an interface in other programming languages. However, Python\u0026rsquo;s dynamic nature has always allowed for a more lenient approach known as \u0026ldquo;duck typing.\u0026rdquo;\nDuck typing is a programming concept where the type or class of an object is determined by its behavior (methods and properties) rather than its class inheritance. While this is a powerful feature, it can lead to issues if the expected behavior is not clearly defined or understood.\n# Duck Typing Example in Python class Bird: def make_sound(self): print(\u0026#34;Some generic bird sound\u0026#34;) class Duck(Bird): def make_sound(self): print(\u0026#34;Quack\u0026#34;) def animal_sound(animal): animal.make_sound() animal = Duck() animal_sound(animal) # Output: Quack Duck typing is not unique to Python; it\u0026rsquo;s also prevalent in languages like Ruby, JavaScript, and Groovy. However, what sets Python apart is its support for making these implicit contracts explicit through protocols and type hints.\nTo mitigate such issues, Python allows for making these implicit \u0026ldquo;contracts\u0026rdquo; explicit through protocols. Here\u0026rsquo;s an example of an implicit form of a protocol:\n# Implicit Protocol Example from typing import Protocol class Drawable(Protocol): def draw(self) -\u0026gt; None: ... # A class adhering to the Drawable protocol implicitly class Circle: def draw(self) -\u0026gt; None: print(\u0026#34;Drawing a circle.\u0026#34;) Protocol Naming Conventions When it comes to naming protocols, adopting an action-oriented convention with an \u0026ldquo;-able\u0026rdquo; suffix (e.g., Drawable, Iterable, Callable) can be helpful. This makes it clear what behavior is expected from a class that adheres to the protocol.\nClass Signatures While Python\u0026rsquo;s flexibility allows you to be implicit about adherence to a protocol, adopting a disciplined approach of being explicit in your class signatures elevates the robustness of your code. Explicitly stating that a class adheres to a protocol removes any ambiguity and assumptions, making the code more straightforward to read, debug, and maintain.\n# Explicitly stating that Circle adheres to the Drawable protocol class Circle(Drawable): def draw(self) -\u0026gt; None: print(\u0026#34;Drawing a circle.\u0026#34;) By taking the extra step to define protocols and explicitly mention them in your class signatures, you create a codebase that is easier to navigate, less prone to errors, and more maintainable in the long run.\nTesting The Non-Negotiable Nature of Unit Testing While various aspects of coding best practices may be subject to interpretation or specific use-cases, the role of unit testing is non-negotiable for writing robust code. At a minimum, \u0026ldquo;happy path\u0026rdquo; testing, which tests the expected behavior under normal conditions, should always be in place. These basic tests not only verify that your code works as intended but also lay the groundwork for future testing. When bugs are discovered, having an existing unit testing framework makes it straightforward to add new tests that capture these specific issues. This ensures that once a bug is fixed, it stays fixed, enhancing the code\u0026rsquo;s long-term reliability.\nUnit Testing in Depth Unit tests are geared towards verifying the smallest units of code in isolation—often individual methods or functions. These tests serve multiple purposes:\nSafety Net: Unit tests catch regressions and errors before they reach production, acting as a first line of defense against bugs. Documentation: Well-written unit tests serve as a form of documentation, clearly outlining what each function or method is supposed to accomplish. Simplifies Debugging: When a test fails, you only need to consider the latest changes, making debugging quicker and easier. In Python, you can write unit tests using the built-in unittest framework, or you can use third-party libraries like pytest for more advanced features and simpler syntax.\n# Example of a simple unit test using unittest import unittest def add(a, b): return a + b class TestAddition(unittest.TestCase): def test_add_positive_numbers(self): self.assertEqual(add(2, 3), 5) def test_add_zero(self): self.assertEqual(add(0, 0), 0) def test_add_negative_numbers(self): self.assertEqual(add(-1, -1), -2) Mocking: An Essential for Isolated Testing Mocking is a powerful technique for isolating the code under test. By replacing external dependencies with mock objects, you can focus solely on the logic you\u0026rsquo;re testing. This is crucial for several reasons:\nIsolation: Mocking external services or modules ensures that your tests are not affected by their behavior or state. Speed: Tests run faster because they don\u0026rsquo;t have to wait for real database queries or API calls. Reliability: Mocking creates a controlled environment for your tests, ensuring that they produce consistent results. Python’s unittest library includes a Mock class for creating simple object mocks. For more fine-grained control over object behavior, libraries like pytest-mock or unittest.mock offer advanced features like spy objects and mock patching.\n# Example of mocking using unittest from unittest import TestCase, mock def fetch_data(api_client): return api_client.get_data() class TestFetchData(TestCase): @mock.patch(\u0026#39;module_name.api_client\u0026#39;) def test_fetch_data(self, mock_client): mock_client.get_data.return_value = \u0026#39;some data\u0026#39; result = fetch_data(mock_client) self.assertEqual(result, \u0026#39;some data\u0026#39;) By making unit testing and mocking non-negotiable parts of your development process, you\u0026rsquo;re not just writing code—you\u0026rsquo;re crafting robust, reliable, and maintainable software.\nDependency Management and Docker In Python projects, dependency management is essential for ensuring consistent behavior across different environments. The requirements.txt file is a widely used method for specifying project dependencies and their versions.\nBest Practices with requirements.txt Pin Versions: Specifying exact package versions ensures that your application behaves consistently across different setups. # requirements.txt with pinned versions Flask==1.1.2 Use Comments: Comments in requirements.txt can describe why a particular package or version is necessary, aiding future maintenance. # requirements.txt with comments Flask==1.1.2 # Needed for web API Virtual Environments Python\u0026rsquo;s native virtual environments, such as those created using venv, are a common way to isolate project-specific dependencies. While effective, they only handle Python packages and not other system dependencies.\n# Creating a virtual environment python -m venv myenv Docker: The Superior Alternative for Isolation Docker containers take environment isolation to the next level. Unlike Python virtual environments, which only isolate Python packages, Docker containers encapsulate the entire application environment. This includes the operating system, system libraries, databases, and other services your application might interact with.\nHere\u0026rsquo;s why Docker should be your go-to choice:\nEnvironment Consistency: Docker ensures your application behaves the same way regardless of where it runs, eliminating the \u0026ldquo;it works on my machine\u0026rdquo; problem. Ease of Deployment: Docker containers can be easily shared, making deployment across different systems or servers straightforward. Microservices: For applications based on micro-services architecture, Docker simplifies service management, as each service can reside in its own container. # Example Dockerfile for a Python application FROM python:3.8 WORKDIR /app COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt COPY . . CMD [\u0026#34;python\u0026#34;, \u0026#34;your_app.py\u0026#34;] By opting for Docker over traditional Python virtual environments, you\u0026rsquo;re not just isolating your Python application; you\u0026rsquo;re making it portable, easy to deploy, and robust, thereby ensuring its reliability and maintainability.\nDocumentation The Unsung Hero of Robust Code While code is written for machines to execute, it is read and understood by humans. Good documentation acts as a guide, making it easier for you (and others) to understand the code\u0026rsquo;s purpose, architecture, and nuances. Without adequate documentation, even the most well-designed codebase can become a labyrinth that is hard to navigate and risky to modify.\nInline Comments and Docstrings Inline comments and docstrings should be the starting point of any documentation effort. Comments can offer context or explain complex pieces of code, while docstrings provide an overview of modules, classes, and functions.\ndef calculate_area(length: float, width: float) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Calculate the area of a rectangle. This function takes the length and width of a rectangle and returns its area. The function assumes that the parameters provided are positive numbers. Zero is considered an acceptable value for either length or width but will result in an area of zero. :param length: The length of the rectangle. :type length: float :param width: The width of the rectangle. :type width: float :return: The area of the rectangle. :rtype: float :raises ValueError: If either length or width is negative. :Example: \u0026gt;\u0026gt;\u0026gt; calculate_area(10, 5) 50.0 \u0026gt;\u0026gt;\u0026gt; calculate_area(0, 5) 0.0 \u0026gt;\u0026gt;\u0026gt; calculate_area(-1, 5) ValueError: Dimensions should be non-negative. \u0026#34;\u0026#34;\u0026#34; if length \u0026lt; 0 or width \u0026lt; 0: raise ValueError(\u0026#34;Dimensions should be non-negative.\u0026#34;) return length * width README Files Every project should have a README file that serves as an introduction to your project. It should contain essential information like what the project does, how to set it up, and how to use it, providing a roadmap to your codebase.\nAPI Documentation For larger projects or libraries, generating API documentation using tools like Sphinx can offer a more organized and navigable format. These documents can be hosted on websites, making it easier for others to understand your code and even contribute to it.\nVersioning Your Documentation As your code evolves, so should your documentation. Versioning your documentation ensures that it stays in sync with your codebase, providing accurate and up-to-date information. This is especially important when your project has multiple versions in use.\nSelf-Documenting Code: A First Line of Defense Before diving into comments, docstrings, and external documentation, your first line of defense should be writing self-documenting code. This practice involves using clear, descriptive names for variables, functions, and classes, as well as making effective use of type annotations. The idea is to write code that is its own best documentation.\nIn many modern programming paradigms, such as iOS development, self-documenting code is highly encouraged. The benefits are clear: it reduces the need for external documentation and makes the codebase easier to understand at a glance.\n# Example of self-documenting code def calculate_rectangle_area(length: float, width: float) -\u0026gt; float: if length \u0026lt; 0 or width \u0026lt; 0: raise ValueError(\u0026#34;Dimensions should be non-negative.\u0026#34;) return length * width In this example, the function name calculate_rectangle_area, the parameter names length and width, and the type annotations collectively provide a good deal of information about what the function does, without requiring the reader to look up external documentation.\nPerformance Optimization The Need for Speed and Efficiency While Python is known for its ease of use and rapid development capabilities, it\u0026rsquo;s not always the fastest language out there. However, with thoughtful optimization techniques, you can often make your Python code run faster and consume fewer resources, contributing to its robustness.\nAlgorithmic Improvements Before diving into code-level optimizations, it\u0026rsquo;s crucial to pick the right algorithms and data structures for your problem. No amount of micro-optimization can make up for an inherently inefficient algorithm.\n# Using a set to check for membership instead of a list # O(1) complexity instead of O(n) my_set = {1, 2, 3, 4, 5} print(4 in my_set) # Faster than using a list Loop Optimization Python offers several built-in functions and syntax that can make loops more efficient. Utilizing list comprehensions and functions like map() and filter() can often speed up iterative operations.\n# Using list comprehension for more efficient looping squared_numbers = [x*x for x in range(10)] Lazy Evaluation Generators and the itertools library can help you work with large data sets more efficiently by utilizing lazy evaluation. This allows you to iterate over data without loading it entirely into memory.\n# Using a generator to yield items one at a time def count_up_to(max): count = 1 while count \u0026lt;= max: yield count count += 1 Profiling and Benchmarking Before and after making optimizations, it\u0026rsquo;s important to profile your code to identify bottlenecks and benchmark its performance. Python\u0026rsquo;s built-in cProfile module or third-party tools like Py-Spy can be extremely useful for this purpose.\n# Using cProfile to profile your Python script python -m cProfile your_script.py When to Optimize It\u0026rsquo;s often said that \u0026ldquo;premature optimization is the root of all evil.\u0026rdquo; While performance is important, it should not come at the cost of readability and maintainability. Always make sure your code is working correctly first, and then optimize the parts that are proven to be bottlenecks.\nBy focusing on performance optimization, you not only make your Python code faster but also more resource-efficient, contributing to its overall robustness and reliability.\nConclusion Writing robust Python code is an art that involves a blend of several key principles: Code Organization, Error Handling, Testing, Version Control, Dependency Management, Documentation, and Performance Optimization. Each of these elements serves as a pillar that upholds the integrity, maintainability, and efficiency of your codebase. As Python continues to permeate various sectors like web development, data science, and machine learning, the universal value of these principles cannot be overstated.\nRemember, robust code is not just about preventing errors but also about making your codebase resilient, understandable, and extendable. In essence, you\u0026rsquo;re not just writing code; you\u0026rsquo;re crafting software that is built to last and to adapt to future needs.\nNow that you\u0026rsquo;re armed with these best practices, the next step is to implement them in your own projects. Start by reviewing your current codebase, identifying areas that could benefit from these practices, and make the necessary adjustments. Challenge yourself to go beyond the basics, delve into additional resources, and contribute to a culture of coding excellence. Your future self—and anyone else who interacts with your code—will thank you.\nAdditional Resources To further enhance your Python coding practices, here are some additional resources you might find helpful:\nBooks: \u0026ldquo;Clean Code: A Handbook of Agile Software Craftsmanship\u0026rdquo; by Robert C. Martin\n“Robust Python” by Patrick Viafore\n","permalink":"https://blog.oll.is/posts/2023-09-04_robust_python/","summary":"A comprehensive guide on crafting robust Python code, focusing on best practices from code organization to performance optimization. Learn the art of writing software that\u0026rsquo;s built to last.","title":"How to Write Python Code That Doesn’t Break: A Guide to Robustness"},{"content":" Introduction In today\u0026rsquo;s digital age, data architects face the complex challenge of navigating various international data privacy laws. This is crucial not only for compliance, but also for designing effective and secure data architectures. This blog post aims to elucidate key data regulations in the United States (specifically California\u0026rsquo;s CCPA), Canada, Puerto Rico, the EMEA region, the APAC region, and Brazil that every data architect should be cognizant of. To provide a relatable point of comparison for U.S.-based architects, we will contrast each of these international laws against the California Consumer Privacy Act (CCPA). While going deeply into each law will out of scope for a single post, I hope to provide a basic understanding of how data protection laws differ once outside of the US for companies looking to expand their reach.\nRegulations United States: California Consumer Privacy Act (CCPA) Intro: The CCPA is a state statute intended to enhance privacy rights and consumer protection for residents of California, United States. Although it is a state law, its reach is effectively nationwide, as many companies in the U.S. opt to comply with it to maintain a uniform data protection policy. Understanding CCPA is crucial as it serves as a reference point for comparing other international data privacy laws.\nImplementation: To adhere to CCPA, data architects need to focus on consumer rights, including the right to know what data is being collected, the right to delete personal data, and the right to opt-out of the sale of personal data. Implementation often involves creating user interfaces for data access, deletion requests, and opt-out options, usually facilitated through website pop-ups or user account settings.\nKey Points for Data Architects:\nConsumer Rights: Include features that allow consumers to view, delete, or opt-out of data collection, as well as correct inaccurate personal information. Data Inventory: Maintain a catalog of the types of personal information collected, used, or stored. Privacy Policy: Keep an updated privacy policy that clearly outlines how consumer data is used and protected. Canada: PIPEDA (Personal Information Protection and Electronic Documents Act) Intro: PIPEDA is Canada\u0026rsquo;s federal privacy law that governs the collection, use, and disclosure of personal information by private-sector organizations. While similar in many respects to the U.S.\u0026rsquo;s CCPA, it has its own unique set of requirements that data architects must understand.\nImplementation: To comply with PIPEDA, data architects should implement mechanisms for explicit user consent, typically in the form of a pop-up or opt-in checkbox. Data encryption and user authentication features are also essential to ensure data security.\nSimilar to CCPA:\nBoth focus on consumer protection and the right to access and correct personal information. Different from CCPA:\nRequires explicit consent for data collection, which is not strictly mandated by CCPA. Key Points for Data Architects:\nData Collection and Consent: Explicit consent is often required, similar to a CCPA-compliant pop-up for user opt-in or opt-out. Data Storage and Security: Secure storage solutions may be needed, in line with CCPA requirements for data security. Data Access and Correction: Features for data retrieval and editing are required, similar to CCPA\u0026rsquo;s consumer rights. Puerto Rico: PR Act 111 Intro: While Puerto Rico is a U.S. territory and thus subject to federal laws like the CCPA, it has additional local regulations under Act 111. This law puts further emphasis on consumer protection, particularly in the context of data breaches.\nImplementation: Data architects in Puerto Rico should focus on stringent data breach detection and notification systems. An up-to-date privacy policy that aligns with both CCPA and Act 111 is also crucial.\nSimilar to CCPA:\nBoth require a written privacy policy. Different from CCPA:\nPR Act 111 has more stringent data breach reporting requirements. Key Points for Data Architects:\nData Privacy Policy: A written policy is mandatory, aligning with CCPA. Information Security Standards: Encryption or other secure storage mechanisms may be required. Regulatory Oversight: Being aware of both local and federal regulations, including CCPA, is crucial. EMEA: GDPR (General Data Protection Regulation) Intro: The GDPR is a comprehensive data protection law that applies across the European Union and impacts any organization that deals with EU citizens\u0026rsquo; data. It sets a high standard for data protection and influences data laws worldwide.\nImplementation: To align with GDPR, data architects need to embrace the principle of \u0026lsquo;Data Protection by Design and Default.\u0026rsquo; This involves integrating data protection features from the start, including strong encryption and user consent mechanisms, often in the form of explicit opt-in checkboxes or pop-ups.\nSimilar to CCPA:\nBoth GDPR and CCPA focus on consumer rights, including the right to access and delete personal data. Both require companies to disclose their data collection practices. Different from CCPA:\nGDPR has stricter consent requirements and imposes data localization rules that CCPA does not. Penalties under GDPR can be significantly higher. Key Points for Data Architects:\nData Protection by Design and Default: Incorporate data protection measures from the ground up. Lawful Processing of Data: Consent pop-ups or opt-in mechanisms are frequently required. Data Subject Rights: Systems must allow for easy access, correction, and deletion of user data. Data Minimization and Storage Limitations: Organizations may only process as much data as absolutely necessary, and can only store the data for as long as necessary for its specified purpose. APAC: Varies by Country (e.g., PDPA in Singapore) Intro: The Asia-Pacific (APAC) region has a diverse landscape of data protection laws, making it a complex area for data architects. Laws vary by country, each with its unique set of regulations and compliance requirements. Due to the scope, a separate post just comparing the APAC will be needed.\nImplementation: Due to the varied nature of APAC regulations, data architects must be well-versed in each country\u0026rsquo;s specific laws if operating across the region. This may involve implementing localized consent mechanisms and data storage solutions.\nSimilar to CCPA:\nMany APAC countries focus on consumer rights to access and correct data. Some countries require disclosure of data collection practices. Different from CCPA:\nVarying levels of data localization requirements exist in APAC countries. The regulatory landscape is fragmented, each country having its set of laws. Key Points for Data Architects:\nConsent and Notification: Localized consent mechanisms may be required. Data Localization: Be aware of specific country requirements for data storage. Cross-border Data Transfer: Ensure compliance when transferring data across borders. Brazil: LGPD (Lei Geral de Proteção de Dados) Intro: Brazil\u0026rsquo;s LGPD is often considered the country\u0026rsquo;s equivalent to Europe\u0026rsquo;s GDPR. It aims to standardize and strengthen the protection of personal data across Brazil.\nImplementation: To comply with LGPD, data architects may need to appoint a Data Protection Officer (DPO) and should implement robust systems for user consent, data access, and data deletion.\nSimilar to CCPA:\nBoth LGPD and CCPA provide consumers with the right to access, correct, and delete their data. Both require transparency in data collection practices. Different from CCPA:\nLGPD has specific requirements for appointing a Data Protection Officer (DPO). LGPD includes a broader set of legal bases for data processing. Key Points for Data Architects:\nData Subject Rights: Features for easy access, correction, and deletion of user data are needed. Data Processing Principles: Consent is regularly needed for data collection, which may require a pop-up or opt-in mechanism. Legal Basis for Processing: Be aware of the various legal bases for data processing under LGPD. Conclusion Navigating the labyrinth of international data regulations is a formidable but essential task for data architects. This post aimed to demystify some critical regulations, including the CCPA in the United States, PIPEDA in Canada, PR Act 111 in Puerto Rico, GDPR in the EMEA region, varying laws in the APAC region, and Brazil\u0026rsquo;s LGPD. Although to cover each to their legal extent a post for each would be necessary. By contrasting each regulation with the widely adopted CCPA, we offered a relatable point of reference for U.S.-based professionals. From consumer rights to cross-border data transfer, the complexity, and scope of compliance cannot be understated. The accompanying comparison table serves as a quick reference guide, summarizing the core aspects of these regulations. Understanding these laws is not just a compliance necessity but a cornerstone for building secure, efficient, and user-centric data architectures. Continue to check back as I continue to dive into the work of Internationalization of Data across the globe.\nSummary Comparison Table of Data Regulations Regulation Consumer Rights Explicit Consent Data Localization Data Breach Reporting Penalties Data Subject Rights Cross-border Data Transfer Regulatory Oversight CCPA Yes No No Yes Moderate Yes Yes FTC \u0026amp; State AGs PIPEDA Yes Yes No Yes Moderate Yes Yes Privacy Commissioner PR Act 111 Yes No No Yes Moderate No Yes Dept. of Consumer Affairs GDPR Yes Yes Yes Yes High Yes Restricted Data Protection Authorities APAC (Varies) Varies Varies Varies Varies Varies Varies Varies Varies LGPD Yes Yes No Yes Moderate Yes Restricted National Data Protection Authority (ANPD) Additional Resources CCPA official website GDPR guidelines PIPEDA guidelines PR Act 111 LGPD overview ","permalink":"https://blog.oll.is/posts/2023-09-01_comparison-of-different-idr/","summary":"A high level summary of Data Regulations including CCPA, PIPEDA, PR Act 111, GDPR, and LGPD","title":"Comparison of Different International Data Regulations"},{"content":" If you are someone who loves to cook, you know how fun it can be to experiment with different recipes and ingredients. But what if you could take your culinary skills to the next level with a little help from AI? That is precisely what I did when I decided to make banana bread with the assistance of ChatGPT.\nFor those who do not know, ChatGPT is an AI model created by OpenAI that generates text based on prompts provided by users. It has been trained on a massive dataset of text, allowing it to generate responses that are often impressively human-like. While ChatGPT has many uses, I was curious to see how it could help me come up with a banana bread recipe and make edits to that recipe as needed.\nFinding the Banana Bread Recipe To get started, I input my initial request for a banana bread recipe into ChatGPT. I was impressed by the speed of the response, as well as the easy and detail of the recipe.\nOne of the things I appreciated about ChatGPT was that it allowed me to easily modify the recipe to suit my needs. For example, I wanted to use vanilla beans instead of vanilla extract, so I asked ChatGPT how I would make this substitution. It was able to provide me with clear instructions on how to use the beans, and the recipe steps updated automatically to reflect this change.\nEditing the Recipe with ChatGPT Another way that ChatGPT helped me was by allowing me to make other edits to the recipe as needed. For example, I did not have a loaf pan available, so I decided to use the smallest pan I had, an 8×8 pan. ChatGPT was able to update the cooking time and temperature to support my new pan size, and it even informed me of how the new pan size would affect the texture of the bread. This kind of personalized assistance really helped me feel more confident in my ability to make the recipe successfully. As well-informed me the benefits/cost of using different size pans.\nPairing Sides with Banana Bread Once I had my banana bread recipe sorted out, I decided to see if ChatGPT could help me come up with some side dishes to serve alongside it. To my delight, it was able to generate a list of options that would pair well with the bread. I was particularly pleased with the healthy options that it presented, as I am continually investigating how to eat more nutritious foods.\nConverting Recipe to Weights Finally, ChatGPT was capable of converting the recipe to weights for me, which was a huge help. By using a single bowl and a scale to measure my ingredients, I was able to save on dishes and streamline the preparation process. This might seem like a small thing, but it really did make a difference in terms of how smoothly everything went.\nConclusion All in all, I was extremely impressed with how well ChatGPT could assist me in making banana bread. Not only was it able to help me find a recipe, but it also allowed me to make various edits to that recipe based on my needs and preferences. All in a quick chat that only took a minute or two. The fact that it was able to generate a list of side dishes and convert the recipe to weights was just icing on the cake (or should I say, frosting on the banana bread!). If you are someone who loves to cook and is curious about the ways that AI can enhance your kitchen experience, I highly recommend giving ChatGPT a try. You might be surprised at just how helpful it can be! I know my daughter was thankful for its help and being able to come home to fresh banana bread.\nBanana Bread Recipe Ingredients:\n3 ripe bananas, mashed 75g (2.6 oz) melted butter 1 teaspoon baking soda Pinch of salt 150g (5.3 oz) sugar 1 large egg, beaten 1 vanilla bean pod 180g (6.3 oz) all-purpose flour Instructions:\nPreheat the oven to 350 °F (ca. 177 °C). Grease an 8×8 inch (ca. 20 cm) baking dish. In a large mixing bowl, combine the mashed bananas and melted butter. Add baking soda, salt, and sugar, and mix well. Split the vanilla bean pod lengthwise with a sharp knife, and use the back of the knife to scrape out the seeds inside the pod. Add the vanilla bean seeds to the banana mixture and stir well. Add the beaten egg and stir to combine. Add the flour and mix until just combined. Do not over-mix. Pour the batter into the greased baking dish and smooth out the top. Bake for 35–40 minutes, or until a toothpick inserted into the center of the bread comes out clean. Allow the bread to cool in the pan for 5–10 minutes before transferring it to a wire rack to cool completely. Recommended sides (verbatim from ChatGPT): Banana bread is delicious on its own, but it also pairs well with a variety of different foods and beverages. Here are some ideas for what to serve with banana bread:\nButter or cream cheese: A spread of butter or cream cheese can add richness and a bit of tang to the bread. Fresh fruit: Sliced bananas, strawberries, or blueberries can be a great compliment to the sweetness of the banana bread. Nuts: Chopped walnuts, pecans, or almonds can add crunch and texture to the bread. Yogurt: A dollop of plain or vanilla-flavored yogurt can add a creamy, tangy contrast to the bread. Coffee or tea: A cup of hot coffee or tea can help to balance out the sweetness of the bread and enhance the flavors. Ice cream: A scoop of vanilla or caramel ice cream can be a decadent dessert option to serve with banana bread. These are just a few ideas to get you started, but please don\u0026rsquo;t hesitate to experiment and find your own favorite pairings!\n","permalink":"https://blog.oll.is/posts/2023-02-15_banana_bread_by_chatgpt/","summary":"ChatGPT has been shown to help with many things. Let us take a look about how ChatGPT can help us in the kitchen, and more importantly if it can make a good banana bread.","title":"Banana Bread by ChatGPT"},{"content":" The Why \u0026amp; Overview Becoming a Staff Engineer as a self-taught developer has been one of my greatest achievements. It has also been my hardest one. In particular, learning how to learn, stay on track, and accomplish goals that typically only come at the end of a boat of failing were quite the hurdles to get over.\nI found creating yearly goals that only really saw quarterly focus as my best way of getting things done. This is normally done in a notebook. Of course, checking back in ever 3–6 months on your progress is important for making sure you are still on the trajectory to hit your targets and make any adjustments that are needed.\nHowever, in the last year or so I have gotten off this system, and it has not been going well for me. I am constantly pulling myself in different directions, allowing myself to get a bit done all over the place, and accomplishing nothing I set out to do. The issue with my old system is that when you are in the middle of an ocean of desires and task you want to do, you have no tools to find your way to shore and get to work.\nI decided this year to try a new system using some methods on how we did large quarterly planning on some projects. This blog post goes into the system step by step on how I got all my thoughts together and set out on ambitious goals that I can take one day at a time.\nFor the purposes of keeping my private goals private, all goals referenced and shown here are from a demo document I made with random goals I thought people would relate too.\nGetting all our goals on the table Our first step is just getting all the goals and things we want to accomplish on the table. Don’t worry about numbers and what you think would be achievable in just this year.\nOur “epics” are just a quick wording of the goal we would like to achieve.\nThe next step is to trim this list down to what we think we could get done in a year. I like to think about what each goal might need, some things like “get healthy” could be as simple as spending time with a nutritionalist and setting time aside for the gym. Other goals like “Learn Arduino” could be more complex and time-consuming, so we might want to limit the amount of those goals on our board.\nIf you are not certain, leave them on for now; our next steps will help figure out time commitments, and we can see if we will be over-worked and if more need to be moved.\nMoved where, you might ask. To our holding area for next year.\nI love this table because it gives me a place to add my random “I would like to learn this” that pop into my head. I can set it aside and next year they can be in the running for how I would like to improve myself for the following year.\nNot only that, but I find it helps to clear your working memory. Once I write down a desire or goal, I can drop it from my mind and be less distracted.\nCreating a 10,000’ Overview To create time estimates and our timeline, we need to think about the problem at hand. We now want to create a detail card for each of our goals we have not tabled. In this, you will want to spend time thinking about the over arching goal, what it will take to accomplish or what is required to do such a thing. For example, reading 40 books, you can calculate this as 3.33 books/month. Now we can ask how realistic this goal is for us, or if it should be changed.\nThe main goal of this step is to try to put into perspective what we would need to do to accomplish this goal. What limits we want to put on the goals; spend time thinking about what the success of this goal looks like.\nEstimating Time Commitment and Order Now we will work on ordering our project using the understanding of our goals and what it means to them to be achieved. I break this part down into 3 distinct steps.\nAssumed Time Commitment (ATC) Our ATC is important for figuring out how things stack up in the world of quick wins vs. marathons. You might ask how “Get Healthy” has less of a time commitment than creating a budget.\nFirst thing, for some long term, passive, goals we might want to only consider the time it takes to create the system, or daily time requirement. Using our Overview of Steps (OoS) our budget goal requires doing an in-depth review of bank statements, building a budget, and finding/interviewing financial advisers. This is in comparison to the “get healthy” goal where we are going to find a nutritionalist, get a gym membership, and “take it one day at a time”.\nWhen comparing the OoS for each goal while Getting Healthy will be a lifelong endeavor, it set up cost seems at least on the surface to be less than creating a budget.\nWhat happens when getting healthy actually ends up taking more time than a budget? Nothing, we make our adjustments, but this process is not about being accurate, it’s about getting you unstuck a moving. We will be wrong on our estimates, just keep moving and work towards a better you.\nLevel of Desire (LoD) The naming really tells us a lot about this. For this step, do not worry about ATC or the OoS; order the goals in order of what excites you most to get done to what sounds too most boring.\nIf a couple of items tie, flip a coin or whatever you need to do to not be blocked. This should be one of the quickest steps in this process. Let your heart take the lead on this step.\nEstimated Time Commitment (ETC) Let me reiterate what I have already said above. Do NOT let the fear of estimating the wrong time block you!\nOn this step, we want to take our knowledge from OoS and ATC to figure out how long goals might take to complete. I like to put them in just 4 buckets 1, 3, 6, 12 moths. If a goal is going to take 2 months, throw it in the quarter bucket. Most goals will take longer than you are expecting, so going one higher won’t hurt you.\nThis step really helps put things into perspective for us on if we are over worked. However, using our ATC table even having 3 goals that will take a full year we might see those goals will probably only take a small part of each week they just have to be worked on during the full year.\nAt this point, if you feel there is too much on the board, move things over to “tabled for next year.”\nThis is also a good area to let time limitations, something like “Make an App” is open-handed and at a large risk of scope creep. Here we decide to limit our time on this goal to one quarter. Whatever app we decide to make for this goal will need to be small enough to fit into this timeline. As they say, meetings will inflate to take up the full scheduled time. Goals are also similar, large, open-ended goals, with no completion date, have a habit of never getting done.\nLet this tool be your leverage to keep goals down to earth and keep scope creep out of your personal growth. If at the end of making the app you have numerous things you would like to add, put in a “change order.” See if you might be able to make a mid-year goal change down the road, but this goal is set.\nVisualizing our year Our timeline is easy and quick to create thanks to our last step.\nHere, we can use our LoD to help order goals that will take less than a year to create. I am a visual person, so having this also helps me see if I’m going to be overwhelmed at the number of things I am asking myself to get done.\nThis is also a great place to consider timing of goals. Something like “Setup Garden” should probably not take place in February if you live up north like I do.\nAgain, if at this point you feel you might have too much going on, table a/n item/s for next year.\nMonthly Execution of Our Goals Before the start of each month, you should create a new area where each goal of that month has a place to be worked on.\nI like to create a sticky-note to-do list where the items I intend to get done that month for each goal are detailed. At the start of the month, this will not be a complete list. Items are continually updated any time I have a new to-do that needs to be done. If I don’t intend to get it done this month and the goal is a multi-month goal, I start the next month\u0026rsquo;s list and put the to-do in that new area.\nFor a longer-term goal, you might consider making your to-do list for the full time you estimated the project to take. This way you can plan how you see the next few months playing out and make sure you are working on enough each month to try to meet your estimated deadline.\nYou can also work on this in a to-do app like “reminders.” I prefer having it all in one place.\nThings change Things will not always go to plan. Maybe a goal ended up not working out this time around, and you decide it is best to table the goal for another time. In this case, I recommend keeping the data you already created for the goals. Just gray out every cell related to the goal, move the goal to “tabled for next year”, and I like to add a red sticky to the OoS sticky with an explanation of what happened.\nThis way, when I am reviewing things at the end of the year, I can reevaluate this goal and figure out if the next year is a good year to try this goal again.\nThanks for Reading As we have talked about earlier in this post, this system is not perfect, things are estimated and assumptions will be made. That is the nature of goal setting. However, with a process in place, we can work on growth in a way that will help us see the accomplishment of many more goals than if we are working in an unorganized manner.\nThis is my first year giving this process a try. I’m sure things will grow and change in time. I hope a look into my system gives you ideas and a spark to aid in creating the “you”, you have always wanted to be.\nAlthough there are many virtual planning boards out there, I created mine with “Freeform” Apple’s new software added to iOS/iPadOS 16.2 and macOS 13.1.\nThere is no way to really share a template outside just sharing a collaboration link. For now, this can be found here, if this link become problematic I might have to remove the document.\n","permalink":"https://blog.oll.is/posts/2023-01-23_organizing-goals-like-software/","summary":"Yearly goal planning is hard, but if you plan on major growths in your life, it can be necessary. Here is my latest method for multi-goal planning to keep things focused and on track.","title":"Organizing Goals like Software Projects"},{"content":" So I had an interesting user story the other day. I was asked to implement a new system that would allow users to pay for and download some of our templates.\nThis is all pretty well and easy thanks to Stripe and their Rails API. However we needed a way to allow users to download the templates using a key that expires a week after purcahse and doesn’t provide the user with the storage URL of the template. So thats what I wanted to show a way you can provide the download without giving away the proverbial cow.\nStarting off with this we are going to assume a few things. We are using Ruby on Rails, Uploading is done via CarrierWave (although most of what we will show it doesn’t matter), and we are using Pundit for Authorization.\nLet’s first talk about the routes we will be working with. Our coding is pretty easy. For the scope of this post we will just focus on the download page logic and nothing to do with purchaing.\n/template/:id/download/:purchase_key # Secured page with download buttons /template/:id/download/:purchase_key/[pdf/psd] # route responsaible for serving up the requested documentresources :template, only [:show, :update] do resources :download, only [:show] do collection do get \u0026amp;#39;:id/pdf\u0026amp;#39; =\u0026amp;gt; download#pdf, as: :pdf get \u0026amp;#39;:id/psd\u0026amp;#39; =\u0026amp;gt; download#psd, as: :psd end end end With the routes shown above we should be linked to our controller where all the action happens. First thing we will want to do is ensure the use has a valid key to even be on our downloads page.\nbefore_action :find_purchase def find_purchase unless (@purchase = PurchaseRecord.find_by(key: param[:id]) flash[:error] = \u0026amp;#34;Can\u0026amp;#39;t find purchase.\u0026amp;#34; redirect_to template_path end end\nThis is a quick and easy check for the purchase record and if one is not found we just send the user somewhere else. However if one is found we make it an instance varable so we have access to it in the rest of our calls.\nIt’s also worth noting we will be using pundit for authorizing the downloads. All pundit is doing is verifying the purchase experation date has not passed if so we do a simlar redirect like above but with a flash message that reflects the expired key.\nNow for the part you have probably scrolled down to the bottom to get to as my rambling has gotten the best of you.\nTo do the secure download we can use a few different methods. The easiest would be either send_data or send_file. I prefer send_data as we can just stream the data to the user. If we go with the send_file method we would need to download the template to our server via a tmp file then send that over. To me this seems like wasted stress on the system.\nAs we have two methods that need to use simliar code I decided to dry it out with its own method that they both can call. An example for the PSD would be. def psd authorize @purchase send_template @purchase.psd_template.url, @purchase.psd_template_identifier end\nThis sends our private url and the file name over to the method send_template to handle the buffer. Sending via the buffer is extremely easy. def send_template(url, filename) data = open(url) send_data data.read, filename: filename, disposition: \u0026amp;#39;inline\u0026amp;#39;, stream: \u0026amp;#39;true\u0026amp;#39;, buffer_size: \u0026amp;#39;4096\u0026amp;#39; end\nWith this method we now have an easy way to send the file over without the user ever seeing the real url of the file. While we take a small hit to facilitate the download over all its not to bad for the security of the file. If we also decide for more analitics we can also add in a tracker to the psd and pdf methods so we can track how often they are being downloaded and get some alalitics on our downloads that we couldn’t get before with the old method of giving the users links that we didn’t control.\nAs a bonus if you are using CarrierWave with something like S3 storage you can override the fog_public method in your PDF/PSD uploaders to make the files private in your bucket to only allow your server to grab them. def fog_public false end\nBy putting this method in just your uploader your other uploaders like avatar and general images wont be locked down as they would be if you made this change in the CarrierWave documentation.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey of software development. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2017-02-22_offering-downloads-without-giving-away-the-cow/","summary":"So I had an interesting user story the other day. I was asked to implement a new system that would allow users to pay for and download some of our templates.\nThis is all pretty well and easy thanks to Stripe and their Rails API. However we needed a way to allow users to download the templates using a key that expires a week after purcahse and doesn’t provide the user with the storage URL of the template.","title":"Offering downloads without giving away the cow"},{"content":" I have found while I really enjoy programming full time. It has also brought on some anxiety I was not expecting. I had been up until July a full time IT administrator for a little over 5 years. In this job while still learning a lot the majority of what I did on a day to day basis was memorized and required little to no effort.\nIt has been string moving into a job that while I still knew enough that after a trial run with the company they still wanted to hire me. I find my self outside my knowledge sphere on a daily basis. I love this as I’m currently learning so much. I have a few blog post / videos on the way about what I’ve learned in ActionMailer and Google Client API for Ruby as a result of what all I have been learning.\nHowever, its still an odd place to be in where everyday you don’t feel like you know enough because you are learning so much and reading a lot of documents. I did feel a bit better after talking to my boss and he mentioned I had been doing great work on getting a feature implemented. As well, while listening to the Talk Python to Me podcast. They had mentioned that even after being on the job for 3 months in most projects you still would be new and not know good portions of the code base.\nMain thing I’m trying to get at for new developers (like me) is, it’s okay. Its a new job and I’m sure you are doing a great job. Even if you feel like you should have caught that typo 30 minutes ago and want to pull your hair out. As long as your learning from all the ups and downs you’ll be okay.\nAs well you might not be in the same boat as me. I am working my first job remotely so I don’t have day to day interaction with developers and 60%-70% of my communication is done via text, with me on a timezone difference of 3–6 hours depending on who I need to talk to.\nI’ll end on this note though, I’m so happy to be in a position that I am learning everyday, doing what I love. I wouldn’t trade it for any other line of work.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-08-04_fears-of-a-new-job/","summary":"I have found while I really enjoy programming full time. It has also brought on some anxiety I was not expecting. I had been up until July a full time IT administrator for a little over 5 years. In this job while still learning a lot the majority of what I did on a day to day basis was memorized and required little to no effort.\nIt has been string moving into a job that while I still knew enough that after a trial run with the company they still wanted to hire me.","title":"Fears of a New Job"},{"content":"Making your first push to a project can be a daunting task if you are new to git. Today we are going to review how I do my checks before pushing and what you can do to avoid making mistakes prevalent to newbies.\nFor me my first step is always running a git status. I do this so I can get a quick list of files that have been added/modified/deleted. When I code I might put a binding.pry in a file I made no other edits to for troubleshooting and if I see the file in the list it's a good reminder to open the file and remove my changes.\nNext we jump into a more detail view git diff, will show you all the changes git has registered. Review each line in detail. Make sure you don't have any commented out code that needs removed, or anything like binding.pry that needs removed. What you see in git diff is what will show on your pull request so make sure you won't be embarrassed by anything on it.\nNow that we have all out changes staged and looking good I like to run git add . and git commit -am \u0026amp;#34;\u0026amp;lt;message\u0026amp;gt;\u0026amp;#34; the -a on git commit will make sure you remove any deleted files.\nDon’t do your push just yet, we need to make sure nothing has changed on the master branch that could break our code. For this I take a multistep approach, although it could be narrowed down a bit to be more efficient. First I jump back to my local master branch git checkout master then I pull down the remote master git pull.\nIf changes were pulled down follow this next step otherwise jump to the next paragraph. I then jump back over to my working branch git checkout nic/working-branch and then run a git merge master. The merge will let you know of any conflicts, open any conflicted files and handle the issues as they come up. To leave the commit message screen that opens in vim by default hit :wq and enter on your keyboard. Now our run your test one more time to make sure no changes broke your code.\nAt this point we have all our changes committed locally and our branch is up to date with everything on the master branch. Next we just need to push the code up to the repository. Best practice it to push the code using your branch so project controllers can test your code easily. git push origin nic/working-branch will push our working branch up to the origin. From here we can log into github and build our pull request!\nSo to summarize with a quick list.\ngit status and git diff git add . and git commit -am ‘message’ git checkout master and git pull git checkout nic/working-branch and git merge (if needed) git push origin nic/working-branch login to github and make pull request As always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-06-06_making-your-first-push-in-git/","summary":"Making your first push to a project can be a daunting task if you are new to git. Today we are going to review how I do my checks before pushing and what you can do to avoid making mistakes prevalent to newbies.\nFor me my first step is always running a git status. I do this so I can get a quick list of files that have been added/modified/deleted. When I code I might put a binding.","title":"Making your first push in git"},{"content":"Today I wanted to go over Blocks, Procs, and Lambdas in Ruby via a easy and short over view. There are a lot of post on the subjects already so I am hoping to just give a high level overview to get your interest peaked on some of the most interesting parts of the Ruby language.\nAbove is my last video going over Blocks and how to use them so I will keep my blocks in here short.\nTo overly simply what these are in a way that will probably upset more senior developers:\nBlocks: Chunks of code that will be ran in the parent method that dynamically changes the logic and results of the method. Procs: Much like Blocks but stored as a variable, and can be passed into functions like a variable Lambdas: A step closer to being full on functions than Procs, but are anonymous in their naming. Ranking them in likely to be used:\nBlocks: Used all the time; Good luck avoiding them Procs: Not used as much, but can be helpful in a when using shared logic Lambdas: Rarely used So jumping into some examples Blocks in conjunction with the yield keyword give you an ability to dynamically change logic happening in the middle of a function. One I’m sure you have come across even if your highly new to programming is the .each function used with arrays or ranges.\nWhat happens here is the .each is managing your loop for you and in each iteration is giving you the current variable its on in the loop and allowing you to do what you want to it. Then when control is handed back to the .each method it continues on with the loop.\nAgain see the video above if you would like to see Blocks in action.\nI like to think of Procs as pointers to a function, although pointers are seem more in languages like C++ I believe it fits because of how they are used. Procs are great ways to pass commonly used blocks around in keeping in line with DRY logic. For example if I wanted to have a Proc that prints out the instance variable I would do something like below.\nmy_new_proc = Proc.new { |x| puts x }\nI can easily use this now in any method that takes a block.\n(0..9).each(\u0026amp;amp;my_new_proc)\nTake note of the ampersand before the variable name. In C++ this would be how you call the address of a variable in memory. This would be why I say this reminded me of a pointer to a function in C++ as it would be used in a very similar manner. All you need to remember though is when using a Proc use an ampersand.\nFinally we will do a quick look into Lambdas. I like to think of Lambdas as anonymous functions much like you see in JavaScript but they are a bit more strict. Two things to know about Lambdas is you can return variables from them, something you can’t do in Procs but can in functions. Next, they are very strict about the number of arguments you pass in. Procs don’t care if you pass in more than you should have, but Lambdas will throw an error if the number of arguments are off.\na = lambda do |x,y| puts x + y return x - y end.call(3,5) console output: 8 a = -2\nSo here we define a lambda by using the keyword lambda and giving it a block of code. To we told it to expect two variables and to output to the console and return a number to a variable. Note we had to chain the method .call to the end of the block to fire the method off. As well we passed in the variables in the call method. This would be because our lambda is not method association and we have no way to call the lambda after its defined so we had to call it after we ended its block of code.\nAs you should be able to see Lambdas are interesting but don’t have much of a place in most the code you will write.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-05-30_blocks-procs-lambdas-oh-my/","summary":"Today I wanted to go over Blocks, Procs, and Lambdas in Ruby via a easy and short over view. There are a lot of post on the subjects already so I am hoping to just give a high level overview to get your interest peaked on some of the most interesting parts of the Ruby language.\nAbove is my last video going over Blocks and how to use them so I will keep my blocks in here short.","title":"Blocks, Procs, Lambdas, oh my!"},{"content":" A nice thing to have in your Ruby journey will be a skeleton project so you can quickly get up and running when you need to create a new project. We have done this project in an earlier post for computers running Windows. However, with the upcoming video series on algorithmic challenges in Ruby I wanted to translate the old post for Mac systems. We will be doing all the commands in the terminal, and the Atom editor for writing to the files.\nFirst we will create our skeleton project folder and its sub folders. mkdir skeleton``cd skeleton``mkdir bin data doc ext lib tests lib/PROJECT\nNow with our folder structure of project in place we will just need to create the template files. The commands are: touch bin/PROJECT``touch lib/PROJECT.rb\nThis will create an places we can start putting our new projects code. Next we will want to make a simple gemspec file that will hold the details of our project. We will name this PROJECT.gemspec and it will be located in our root project directory. touch PROJECT.gemspec``atom ./PROJECT.gemspec\nPROJECT.gemspec file contents: # coding: utf-8 lib = File.expand_path(\u0026amp;#39;../lib\u0026amp;#39;, __FILE__) $LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)``Gem::Specification.new do |spec| spec.name = \u0026amp;#34;PROJECT\u0026amp;#34; spec.version = \u0026amp;#39;1.0\u0026amp;#39; spec.authors = [\u0026amp;#34;Your Name Here\u0026amp;#34;] spec.email = [\u0026amp;#34;youremail@yourdomain.com\u0026amp;#34;] spec.summary = %q{Short summary of your project} spec.description = %q{Longer description of your project.} spec.homepage = \u0026amp;#34;http://domainforproject.com/\u0026amp;#34; spec.license = \u0026amp;#34;MIT\u0026amp;#34;``spec.files = [\u0026amp;#39;lib/PROJECT.rb\u0026amp;#39;] spec.executables = [\u0026amp;#39;bin/PROJECT\u0026amp;#39;] spec.test_files = [\u0026amp;#39;tests/test_PROJECT.rb\u0026amp;#39;] spec.require_paths = [\u0026amp;#34;lib\u0026amp;#34;] end\nAfter the gemspec file is done we will want to create our Rakefile to help us with automating common task when working with Ruby. This Rakefile will be used to help run our test. touch Rakefile``atom ./Rakefile\nRakefile file contents: require \u0026amp;#39;rake/testtask\u0026amp;#39;``Rake::TestTask.new do |t|``t.libs \u0026amp;lt;\u0026amp;lt; \u0026amp;#34;tests\u0026amp;#34;``t.test_files = FileList[\u0026amp;#39;test/test*.rb\u0026amp;#39;]``t.verbose = true``end\nFinally we will want to make a simple test file to finish up our skeleton project touch tests/test_PROJECT.rb``atom ./tests/test_PROJECT.rb\ntest_PROJECT.rb contents: require \u0026amp;#34;./lib/PROJECT.rb\u0026amp;#34;``require \u0026amp;#34;test/unit\u0026amp;#34;``class TestName \u0026amp;lt; Test::Unit::TestCase``def test_sample``assert_equal(4, 2+2)``end``end\nNow we should have our test project all finished up. You can test your project by calling “rake test” from your project’s root directory. If setup correctly you should get a passed test without any failures or errors. Below is the file structure you should have now. skeleton/``PROJECT.gemspec``Rakefile``data/``ext/``tests/``test_NAME.rb``bin/``PROJECT``lib/``PROJECT``PROJECT.rb\nNow when you want to use this to creating a new project all you will need to do is the following.\nCopy the skeleton project and rename the root folder to your new project name Rename the lib/PROJECT.rb file and the lib/PROJECT folder to the new project name Edit the Gemspec file to fit the new project rename and tests/test_PROJECT.rb and make sure to edit the ‘require “./lib/PROJECT.rb”’ inside that test file Double check everything is working by calling “rake tests”. Happy Coding! As always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-05-23_skeleton-ruby-project-mac-edition/","summary":"A nice thing to have in your Ruby journey will be a skeleton project so you can quickly get up and running when you need to create a new project. We have done this project in an earlier post for computers running Windows. However, with the upcoming video series on algorithmic challenges in Ruby I wanted to translate the old post for Mac systems. We will be doing all the commands in the terminal, and the Atom editor for writing to the files.","title":"Skeleton Ruby Project: Mac Edition"},{"content":" One question I hear a lot and have even asked myself a while back ago is “When do I know enough to start looking for a job”. While this question is very complicated and depends on a lot of factors I wanted to go into my background a bit and what got me to the point of saying “I know enough”.\nLike I’ve mentioned in the past I have about 5 years of hobbyist programming under my belt but until recently I have not really had much to show for it. Mostly due to a lack of discipline and not staying in a stack long enough to learn the amount needed to get a job.\nTypically if you look at boot camps and online courses they have you go though and make at least 6 sample projects to show off on your own website. I think this is a great as it gives you a lot of experience and a lot to show off in interviews. My only warning is don’t do the same thing over again. For instances if you are wanting to get into website programming. Make your portfolio, an app that uses API’s like Wikipedia, maybe a small game like [Simon] (http://ollis.me/simon/)(this has gotten praise at interviews before), and more diversity depending on what kind of projects interest you. Same applies if you are looking for Android programming or even Game Programming. If you want a job making mobile apps, first start making apps to show you know your stuff.\nThis will give you a solid foundation, and proof you know your stuff. While you still wont be an expert you should have enough to land an entry job depending on where you are.\nHowever, if you are like me you might still be worried. I have always been odd with my self esteem and worried while I feel I know a lot about programming, would I be able to keep up in a team environment with deadlines?\nThe big vindication for me was participating in my first hackathon. During the hackathon we used the MEAN stack. The only technology I have used in the MEAN stack up to this point was JavaScript and to my surprise we used TypeScript instead of JavaScript so I was really out of water on this. So to give me somewhere I could work I asked to be on the front end, as although I didn’t know Angular JS at the time, I could do HTML, CSS, and limp my way though TypeScript.\nI am happy to say I was able to pickup enough on the technology pretty quickly to contribute features and code to the front end. Being able to be throw into a new tech stack and in 24 hours be able to churn out code was the confidence boost I needed to feel like I could take on a programming job and not completely fail on day one.\nHowever, I do have some warnings with hackathons. Find a team before the event. I was lucky and met a very nice guy at the Ruby meetup in town that was going to be at the hackathon and he got me on his team who had an experience developer who was just looking to have a good time and didn’t mind looking over the new guys.\nWhile I didn’t know the guy from the Ruby meetup other than our one conversation, that was enough to get onto a team that did some great things. I say this because I over heard the people who helped find teams for coders say they put all the “dead weight” on one team to let it sink. Its very sad as I feel hackathons should be for community development more then the outcomes of the projects. A whole team of people excited to get into coding ended up getting a horrible experience that could have very well killed their desire in coding.\nTo wrap up what has turned into a ramble. Pick your platform of choice and make 6 unique apps / sites that all have their own features. Don’t be afraid to clone, they are just example projects. Go to a hackathon, or a local meetup on a night they might be doing a coding project. Main thing is don’t go to a hackathon alone. Ask around a meetup group and see if anyone else is going and if they have room on their team for one more. At the end of the day coding skills are important, but your confidence in learning new skills quickly will determine your success in an ever changing technology world.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-05-16_when-to-start-looking-for-a-job/","summary":"One question I hear a lot and have even asked myself a while back ago is “When do I know enough to start looking for a job”. While this question is very complicated and depends on a lot of factors I wanted to go into my background a bit and what got me to the point of saying “I know enough”.\nLike I’ve mentioned in the past I have about 5 years of hobbyist programming under my belt but until recently I have not really had much to show for it.","title":"When to start looking for a job"},{"content":" Today I wanted to review automating code review inside of ruby.\nTheir meeting great open source projects to help make sure you are sticking to an idiomatic way of coding in ruby. The main one I will focus on today is Robocop and Robocop-git.\n[Rubocop] (https://github.com/bbatsov/rubocop)is an open source gym for ruby that will review your project and let you know of any “best practices” you are not adhering too. As described on the Rubocop github page. Out of the box it will enforce guidelines set by the Ruby Style Guide.\nBest of all, not only will it report problems to you, but using the -a flag it can even auto correct many of the problems it fined.\nRubocop is extremely easy to install just rungem install rubocopfrom there you can easily run rubo cop on your project by just running rubocop in your terminal in the root of your project.\nRubocop can be a great tool for you to run on a project to try an help contribute when you are a new comer and still learning your way around ruby. However, don’t just go editing without thought. Sometime we might use fail instead of rescue because it looks better when reading. However, Rubocop wont know that and still tell you to correct it. This might lead to your efforts of helping out an open sourced project being shot down. Moral of the story, don't get to trigger happy just because Rubocop says it should be changed.\nHowever, one issue is you might be working in a very large project and you don’t want to run rubocop on the whole project due to the time / complexity of the results. You might also just want to run Rubocop on the code changes you made.\nFor this project we have the project Rubocop-git this is just an addition on top of rubocop. However, in this project it will parse your git diff data and only show you issues on the code you have changed. This is great for when you are just wanting to run a quick check on the code you contributed to make sure it wont get sent back for something silly as using some outdated method name / convention.\nI personally have contributed to the rubocop-git project, although at the time of writing this it has still not been pulled into the master branch. I added the autocorrect flag -a found in the normal rubocop project back into the rubocop-git project. If you would like to start using this feature before it gets pulled into the master feel free to download the project on my repo.\nAs a new developer I highly recommend using one of these gems while you are still learning the ropes to help you get productive quickly and not get things shot down because they didn’t conform to the style guidelines.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-05-09_automated-code-review-in-ruby/","summary":"Today I wanted to review automating code review inside of ruby.\nTheir meeting great open source projects to help make sure you are sticking to an idiomatic way of coding in ruby. The main one I will focus on today is Robocop and Robocop-git.\n[Rubocop] (https://github.com/bbatsov/rubocop)is an open source gym for ruby that will review your project and let you know of any “best practices” you are not adhering too.","title":"Automated Code Review in Ruby"},{"content":"Program Practical is now on youtube. This month I am moving to a bi-media format with blog post going to start coming out weekly on Program Practical, as well weekly video post on YouTube.\nThese post will cover a range of programming topics, from progress updates, coding tutorials, and coding news. I will be currently be focused more on Ruby and Rails, but might jump into C++, Python, and other languages depending on my current project or topic request.\nThe current videos in the pipeline include a background on my coding past, some code problem solving in C++, and one I’m shooting tonight on making Linked List in Ruby.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Subscribe to the Program Practical channel on YouTube. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-05-02_live-on-youtube/","summary":"Program Practical is now on youtube. This month I am moving to a bi-media format with blog post going to start coming out weekly on Program Practical, as well weekly video post on YouTube.\nThese post will cover a range of programming topics, from progress updates, coding tutorials, and coding news. I will be currently be focused more on Ruby and Rails, but might jump into C++, Python, and other languages depending on my current project or topic request.","title":"Live on YouTube"},{"content":"We are officially into april now I figured I would do my monthly wrap up and planning blog. March was an okayish month. If you follow along you would noticed I only ended up posting 3 times last month. While I am working on studying a lot I have started looking at developer jobs. With this I’m not posting too much about the places or the journey but it has shaped a bit about what I will be doing this month.\nFirst off to catch up on what was done last month. I have started doing some live streaming although disorganized it’s a start. During the stream I was working on adding a new feature to a gem rubocop-git. That is currently pending via pull request but is available on my page. I have also continued on the rails tutorial but have chosen not to live stream that due to the content and with the reading, it’s not a good stream subject.\nFor the month of april I’ll be switching gears at least for the next few weeks and studying PHP and the Craft CSM. I’m currently working on an opportunity but they use PHP and Craft as their development platform. Once we wrap that up in a few weeks I’m hoping to finish up the Rails tutorial later this month and start working on a new open sourced project I’m pretty excited about and will be live streaming a good amount of its creation.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-04-09_april-showers-bring-in-some-changes/","summary":"We are officially into april now I figured I would do my monthly wrap up and planning blog. March was an okayish month. If you follow along you would noticed I only ended up posting 3 times last month. While I am working on studying a lot I have started looking at developer jobs. With this I’m not posting too much about the places or the journey but it has shaped a bit about what I will be doing this month.","title":"April showers bring in some changes"},{"content":"It’s been a bit between post. With the holidays things tend to get busy with my family. I have started live streaming some coding. I’m hoping to make it a more regular thing, although I’m trying to work it into a fixed point on in my week.\nAfter editing some of the dead points of the live stream I plan to release the video’s on youtube. The current live stream is following me along as I’m adding a new feature to the open sourced project rubocop-git. Currently on the stream it’s me and my friend Alex, who is just getting into program. While he is not busy with work he is helping to ask questions that I might not think about bringing up for newer programmers.\nFor the future I plan to do some more live streams on the other open sourced projects I’m working on and how to contribute to open sourced projects. I am also planning a series this summer when bash comes to windows to see how windows stacks up for Ruby and Ruby on Rails development.\nFor anyone that missed the news yesterday. Windows will be releasing an “anniversary update” or Windows 10.1 this summer. They have partnered with Canonical to bring “a full ubuntu image” into windows as a subsystem. Allowing you to asses bash in a full linux like environment. This should allow for file I/O and threading to happen the same way linux implements it. It’s my hope (and windows) that this will bring development back to windows for many projects and libraries that have no, or broken support for windows.\nThe live streaming will be happening over on YouTube as well I’m hoping to have last night’s stream live soon. It’s a little rough but it’s a start.\nAs always thanks for reading, and follow me on twitter for more daily updates, live stream alerts, and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-03-31_live-streaming/","summary":"It’s been a bit between post. With the holidays things tend to get busy with my family. I have started live streaming some coding. I’m hoping to make it a more regular thing, although I’m trying to work it into a fixed point on in my week.\nAfter editing some of the dead points of the live stream I plan to release the video’s on youtube. The current live stream is following me along as I’m adding a new feature to the open sourced project rubocop-git.","title":"Live Streaming"},{"content":"Well sadly more and more I find myself programming on my Mac over my Windows machine. While I still believe windows is a great platform for programming even in Ruby it’s all come down to one thing. Time. The more projects I work on the more gems I get exposed to and the more errors those gems produce due to not being testing or just no support for windows. While this may be addressable by installing a linux wrapper like cygwin onto the windows system I have not personally gave it a try.\nWhile trying to use the windows machine I am constantly researching error messages that end in one of three ways. I change some config in that gem to fix the issue, rollback gem versions to one that worked on windows, or move over to the mac.\nIt’s unfortunate however it’s understandable. As I mentioned in another blog post, most ruby/rails app are going to be running on linux servers, so why break your back to support a development environment that will have no implementation in production.\nWith time being my main driver it’s more important to get things done then fight with a half supported platform. I have been working on my Macbook Air for a good part of the time. The only issue with it, is it stresses to run on my ultrawide display. However, I have found a home lately on Cloud9. It has provided me a nice web ide to program in, while giving me access to a Ubuntu linux terminal to do all my ruby and rails work in. The biggest issue I have with Cloud9 is just with the IDE itself. I use Atom on my computers and love the keyboard shortcuts and little gems it can do speeding up my programming time. The only other issue I have is speed of the VM. it takes a while to run rake test although I have been told if I upgrade to a paid account you get more power.\nHowever, at the end of the day it does what I need. So if you are looking at getting into rails and can’t afford a mac I’d encourage you to check out Cloud9. Your projects are public however, in your learning just about everything you do should be on github so its not really a big deal. Main thing is don’t let yourself get discouraged by all the errors you are getting. Move to a Linux or Mac environment so you can focus on learning, and not troubleshooting.\nAs always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-03-17_tossing-in-the-towel-for-windows/","summary":"Well sadly more and more I find myself programming on my Mac over my Windows machine. While I still believe windows is a great platform for programming even in Ruby it’s all come down to one thing. Time. The more projects I work on the more gems I get exposed to and the more errors those gems produce due to not being testing or just no support for windows. While this may be addressable by installing a linux wrapper like cygwin onto the windows system I have not personally gave it a try.","title":"Tossing in The Towel for Windows"},{"content":"March is upon us and I just finished up One Month Rails. While I did enjoy the class I have to say even coming from someone who is new to Rails it went insanely slow at the beginning allowing me to finish the whole course in about a week. However, I will say there explanation and run through of a MVC framework was amazing. I have worked in them before but feel I have a better understanding now than I ever had in the past.\nWhile I still feel $45 is completely overpriced for the content, I was happy I got a coupon to take the course for a dollar. I could still see the course being worth $45 though if you are completely new to backend web programming as they do break down some concepts well enough that it should be a good start before going with more advanced material.\nSo what’s the plans for March? I’ve take the first few days of this week off for a break and to deal with the loss of our oldest dog. However, I’ll be starting up with Michael Hartl’s Rails Tutorial for more of an in depth look at rails and away from all the generated code and scaffolding found in One Month Rails.\nI’m excited to get both into this new month of rails programing and get yet another project for the portfolio that is now live!\nAs for meetups this month it is always important to stay connected. We have the Indy.rb meetup this week, and later this month the Bloomington.rb meetup. Hopefully I’ll get to attend the local Open Indy meetup where we hope to continue some of the work from the hackathon. However, with spring break looming over next week we will have to see if the local meetup groups take a hiatus like the rest of the town.\nAs always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below, and don’t forget it’s always a good time to start to #LearnToCode\n","permalink":"https://blog.oll.is/posts/2016-03-08_kicking-off-a-new-month/","summary":"March is upon us and I just finished up One Month Rails. While I did enjoy the class I have to say even coming from someone who is new to Rails it went insanely slow at the beginning allowing me to finish the whole course in about a week. However, I will say there explanation and run through of a MVC framework was amazing. I have worked in them before but feel I have a better understanding now than I ever had in the past.","title":"Kicking off a new month"},{"content":" This last weekend I got to participate in #INCapitolHack. A hackathon to help create solutions for the city of Indianapolis Indiana. This was my first hackathon and I wanted to share some of my experience with other working-to-be developers.\nThe event started Friday night with an overview of the event to come and team creation time. One of the biggest thing I will say is get on a team before the event. This might seem a hard task if you’re like me and are new to social development and only been to a meetup or two. My advice here is go social. I sent out messages on twitter, flowdock, and meetup asking if anyone from the local Ruby group would be attending the hackathon. I ended up messaging a fellow I met at the Bloomington ruby meetup when I saw him RSVP’d to the event on another meetup page. He had also found someone on meetup and we had our team.\nThe reason why I say doing use the help of the hackathon to get on a team is because I overheard the organizers the next day talking about creating teams the day before. In one guy’s words “I just put all the dead weight on one team”. I find this is horrible as hopeful developers can learn a lot of experienced ones in hackathons and while they might be “dead weight” split around evenly you can really help the tech community grow. Something they were claiming to do.\nAs for the mindset of “I don’t know enough to do this”. If you know some front end stuff including HTML and CSS then you can be helpful. Going in I have never touched any part of the MEAN stack, however that is what we were going with. I stayed up late the first night getting my laptop ready for the next day as the team chatted on Slack, and looked through some of the aspects of the MEAN stack.\nThe key here is having someone on your team who is experienced, and allowing them to pick the development stack as they can really be a team leader and help in your 24-hour crash course into this new tech. As well if you created the team early enough before the event you can all be team programing and learning the stack before the event.\nHowever, after the event was over, we might not have one but we made new friends and learned new technology. As well for me I gained the confidence that I can learn quickly and become competent enough in start producing in a short amount of time in a new stack. This has pushed me enough to know I can survive as a Jr. Developer.\nI can’t stress it more, if you’re trying to become a developer, get out and go to hackathons. The experience in team work, deadlines, knowledge, and producing under stress is unmatched in anything else you can get in a short 24-hour timespan.\nAs always thanks for reading find more on my blog at Program Practical, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below.\n","permalink":"https://blog.oll.is/posts/2016-03-02_experience-and-advice-from-a-newbies-first-hackathon/","summary":"This last weekend I got to participate in #INCapitolHack. A hackathon to help create solutions for the city of Indianapolis Indiana. This was my first hackathon and I wanted to share some of my experience with other working-to-be developers.\nThe event started Friday night with an overview of the event to come and team creation time. One of the biggest thing I will say is get on a team before the event.","title":"Experience and Advice from a Newbies First Hackathon"},{"content":" This weekend I have the honor of participating and the #INCapitolHack hosted by the Indianapolis Chamber of Commerce. While I’ll be typing up another post about my experience and what we did. I wanted to take the time to type up what was in my bag for the event, to help new comers feel prepared for going to their first hackathon.\nBeing my first hackathon I wasn’t to sure what I might need or should bring so I made sure to have a little big of everything.\nElectronic wise I was sporting:\n13\u0026quot; MacBook Air (Yes is provides plenty of power for web dev, although I wish for better resolution) iPad Mini (For testing responsive design on an actual mobile browser Nexus 6 (taking photo) Moto360 Watch Tech wise I was pretty covered, I only used my iPad for about 10 minutes, but not knowing the teams goals I didn’t know how mobile we would be set on. I’ll end up bringing it to the next one as things might change.\nElectronic Accessories included:\nRazer BlackWidow TE Keyboard (Green) Apple Magic Mouse LG Mobile Wireless Mouse LG Bluetooth headphones Generic wired ear buds 8GB Flash Drive Google Fi Power Brick Cat 5e cable and Ethernet adapter For the accessories I’m happy to have had my keyboard, I type much better/quicker on it than the cheaper laptop keyboard. As well two mice was a good plan, as my Magic Mouse decided to keep disconnecting from my laptop while I was there so I ended up switching to the LG. Although I was working in a team you go on sprints where you have a job and need to get it done, rocking out to some good tunes helps me keep going so the head phones are a must for me, although I would only keep on bud in at a time. The flash drive and power brick are also great haves, like the iPad things change, although my team was well organized and used GitHub for all of our code sharing and Slack for some quick code sharing I saw teams less organized and having issues with code sharing. We where also lucky to have amazing wifi making pulls and pushes quick and easy. The power brick was nice to have as you never know what the power situation is going to be like and the MacBook Air only has 2 usb ports. As for my Cat5e and adapter I would probably leave those at home next time. However, they are normally just in my laptop bag so I left them in\nNon Tech Accessories:\nMulticolor pin, Black pin My Normal take-with-me notepad Larger notepad used for design work Not much on the non-tech accessories. I’ll end up leaving my larger notepad and MC pin at home next time. One thing I learned in a hackathon is you have no time. design concepts and work is done on the fly. Although my take-with-me notepad will always stay in my bag. I highly recommend one. You get some QA with the judges and other talks worth taking notes in. As well just taking in a new team of people you learn new things and stuff to research later and its good to have something to write it all down in.\n","permalink":"https://blog.oll.is/posts/2016-02-29_inside-my-tech-bag-for-incapitolhack/","summary":"This weekend I have the honor of participating and the #INCapitolHack hosted by the Indianapolis Chamber of Commerce. While I’ll be typing up another post about my experience and what we did. I wanted to take the time to type up what was in my bag for the event, to help new comers feel prepared for going to their first hackathon.\nBeing my first hackathon I wasn’t to sure what I might need or should bring so I made sure to have a little big of everything.","title":"Inside my Tech Bag for INCapitolHack"},{"content":"So far I have stuck to my guns and continued to develop with Ruby on Rails on a Windows machine. I am overall quite happy with my experience although I have ran into some fun roadblocks.\nOne of the biggest issues I have found with developing on Windows has been Ruby Gems not fully supporting it. The first Gem I encountered with issues was “ReRun” a nice little gem that would auto reload my Sinatra app when it noticed files had been changed. While this gem isn’t needed now that I’m on Rails, it completely fails to run on Windows. Further investigation shows a developer who is quick to throw in the towel saying that he has no access to a windows machine so he will not be fixing it. This bothers me for a few reasons but mainly I run a MacBook Air. Even that small laptop runs a windows 7 VM on top of OS X without issues. However, the Gem is open sourced, so I feel the real reason it lacks support is because it lacks interest. However, it’s important to note for Windows developers that not all gems play nice.\nI also wonder as time goes on if gem problems isn’t a good thing. For many new developers one of the challenges I find my friends face is learning to search for solutions. While you can make the argument that it could make new developers shy away from learning to program, I argue that it can help make them better programmers over time.\nOne of the Rubiest from our Indy meetup had mentioned he isn’t against developing on Windows. However, when you are making the money software developers made the cost difference is really minimal to be more productive. After all once you have learned enough to land a job it might be best not to spend 1/2 of your continued learning time troubleshooting windows only gem bugs.\nHowever, I feel that the paywall of learning ruby shouldn’t hang on developers pushing would-be developers to by a who new computer. I’ll be releasing a video series soon for setting up Ruby on Rails on a Windows machine and some of the house cleaning things to make the setup easy. However, for now if you are looking into becoming a Ruby developer and don’t have the cash for a hot new Mac, or don’t want to spend the money on it I have three recommendations.\nGo for it, you will run into many issues on a Mac as well, while it might not be as much it’ll take the same skills to resolve so might as well get some good experience. Go used. I live near a University, its common for them to have outlet stores to sell old equipment. You can pick up 2–3 year old Mac Mini’s for $300 there. Go cloud. Using a service like Cloud9 is amazing because you get a full Ubuntu virtual machine and IDE in the browser. I use this now for some projects to make it easy to go back and forth between my desktop and laptop without having to worry about if I pushed code to GitHub. As always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below.\n","permalink":"https://blog.oll.is/posts/2016-02-24_developing-ruby-on-windows/","summary":"So far I have stuck to my guns and continued to develop with Ruby on Rails on a Windows machine. I am overall quite happy with my experience although I have ran into some fun roadblocks.\nOne of the biggest issues I have found with developing on Windows has been Ruby Gems not fully supporting it. The first Gem I encountered with issues was “ReRun” a nice little gem that would auto reload my Sinatra app when it noticed files had been changed.","title":"Developing Ruby on Windows"},{"content":" It has been a fun month so far. I just wrapped up “Learn Ruby the Hard Way” finishing the two books from “No Degree, No Problem” you read before getting into rails. Next step I’m starting today is “One Month Rails”.\nI reviewed the site last night and almost signed up but as its a monthly charge I decided to wait until the start of my study time today. This actually worked out really well because as I was 1/2 though the checkout process then quick on the credit card screen they sent me a coupon in my email for the first month for only $1 instead of the $45 I was about to pay last night.\nAs for “Learn Ruby the Hard Way” I have to say while I enjoyed the book there are some things that really bothered me.\nFirst off while Zed addresses this issue the code is not written in a way many Rubiest would call normal. It favors python styling of code that while you are still learning Ruby might make your code portfolio questionable by more senior developers. Zed does acknowledge this and even says he should not be your only book and encourages your to look at Ruby coding standards later in the book to start seeing how you should be writing. As well at the time of writing this post Zed is working on the 4th edition of his book that will be rewritten in idiomatic code and “updated content that matches or even better Learn Python the Hard Way”. It will be interesting to review the book once it comes out.\nMy other issues with the book is there is no mention of Rspec, and favor of sinatra. From the people, code, and meetups i’ve been too and seen. Rspec is primarily used for testing in Ruby. Especially if you want to use some mutation testing gems. Instead Zed focuses on Test::Unit for all the testing and while I’m happy to have started learning to test my code I feel its a weakness in my portfolio to upload this non-standardized code and test to github. As well he pushes Sinatra for making a website. While I find Sinatra very easy to get into and no problems there it’s on of those gems that I feel many looking at the project will go, “This is nice but we don’t have anything here that runs Sinatra, where is your Ruby on Rails projects?”\nI still recommend the book has you can view it online for free and it does really break down some hard concepts that I know tripped me up years ago when I was learning to program in C# back in high school. I really hope the 4th edition corrects some of the issues addressed here and it can become a good foundation for not just understanding code but also learning how to properly code in Ruby.\nThis has been a fun month learning Ruby and I can’t wait to get into Rails starting today! As always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below.\n","permalink":"https://blog.oll.is/posts/2016-02-22_onto-rails-and-a-review-of-whats-been-done/","summary":"It has been a fun month so far. I just wrapped up “Learn Ruby the Hard Way” finishing the two books from “No Degree, No Problem” you read before getting into rails. Next step I’m starting today is “One Month Rails”.\nI reviewed the site last night and almost signed up but as its a monthly charge I decided to wait until the start of my study time today. This actually worked out really well because as I was 1/2 though the checkout process then quick on the credit card screen they sent me a coupon in my email for the first month for only $1 instead of the $45 I was about to pay last night.","title":"Onto Rails and a Review of What’s Been Done"},{"content":" A nice thing to have in your Ruby journey will be a skeleton project so you can quickly get up and running when you need to create a new project. For this we will be setting up in a windows environment using Powershell and Visual Studio’s Code callabled by “code”.\nFirst we will create our skeleton project folder and its sub folders. mkdir skeleton``cd skeleton``mkdir bin, data, doc, ext, lib, tests, lib/PROJECT\nNow with our folder structure of project in place we will just need to create the template files. In Powershell the commands are: new-item -type file bin/PROJECT``new-item -type file lib/PROJECT.rb\nThis will create an places we can start putting our new projects code. Next we will want to make a simple gemspec file that will hold the details of our project. We will name this PROJECT.gemspec and it will be located in our root project directory. new-item -type file PROJECT.gemspec``code ./PROJECT.gemspec\nPROJECT.gemspec file contents: # coding: utf-8 lib = File.expand_path(\u0026amp;#39;../lib\u0026amp;#39;, __FILE__) $LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)``Gem::Specification.new do |spec| spec.name = \u0026amp;#34;PROJECT\u0026amp;#34; spec.version = \u0026amp;#39;1.0\u0026amp;#39; spec.authors = [\u0026amp;#34;Your Name Here\u0026amp;#34;] spec.email = [\u0026amp;#34;youremail@yourdomain.com\u0026amp;#34;] spec.summary = %q{Short summary of your project} spec.description = %q{Longer description of your project.} spec.homepage = \u0026amp;#34;http://domainforproject.com/\u0026amp;#34; spec.license = \u0026amp;#34;MIT\u0026amp;#34;``spec.files = [\u0026amp;#39;lib/PROJECT.rb\u0026amp;#39;] spec.executables = [\u0026amp;#39;bin/PROJECT\u0026amp;#39;] spec.test_files = [\u0026amp;#39;tests/test_PROJECT.rb\u0026amp;#39;] spec.require_paths = [\u0026amp;#34;lib\u0026amp;#34;] end\nAfter the gemspec file is done we will want to create our Rakefile to help us with automating common task when working with Ruby. This Rakefile will be used to help run our test. new-item -type file Rakefile``code ./Rakefile\nRakefile file contents: require \u0026amp;#39;rake/testtask\u0026amp;#39;``Rake::TestTask.new do |t|``t.libs \u0026amp;lt;\u0026amp;lt; \u0026amp;#34;tests\u0026amp;#34;``t.test_files = FileList[\u0026amp;#39;test/test*.rb\u0026amp;#39;]``t.verbose = true``end\nFinally we will want to make a simple test file to finish up our skeleton project new-item -type file tests/test_PROJECT.rb``code ./tests/test_PROJECT.rb\ntest_PROJECT.rb contents: require \u0026amp;#34;./lib/PROJECT.rb\u0026amp;#34;``require \u0026amp;#34;test/unit\u0026amp;#34;``class TestName \u0026amp;lt; Test::Unit::TestCase``def test_sample``assert_equal(4, 2+2)``end``end\nNow we should have our test project all finished up. You can test your project by calling “rake test” from your project’s root directory. If setup correctly you should get a passed test without any failures or errors. Below is the file structure you should have now. skeleton/``PROJECT.gemspec``Rakefile``data/``ext/``tests/``test_NAME.rb``bin/``PROJECT``lib/``PROJECT``PROJECT.rb\nNow when you want to use this to creating a new project all you will need to do is the following.\nCopy the skeleton project and rename the root folder to your new project name Rename the lib/PROJECT.rb file and the lib/PROJECT folder to the new project name Edit the Gemspec file to fit the new project rename and tests/test_PROJECT.rb and make sure to edit the ‘require “./lib/PROJECT.rb”’ inside that test file Double check everything is working by calling “rake tests”. Happy Coding! As always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer. Feel free to ask any questions in the comments below.\n","permalink":"https://blog.oll.is/posts/2016-02-16_creating-a-ruby-skeleton/","summary":"A nice thing to have in your Ruby journey will be a skeleton project so you can quickly get up and running when you need to create a new project. For this we will be setting up in a windows environment using Powershell and Visual Studio’s Code callabled by “code”.\nFirst we will create our skeleton project folder and its sub folders. mkdir skeleton``cd skeleton``mkdir bin, data, doc, ext, lib, tests, lib/PROJECT","title":"Creating a Ruby Skeleton"},{"content":"It’s been a pretty crazy February for me. However, I wanted to take some time just to catch up on progress and whats going on.\nI’m just passed the 1/2 mark of “Learn Ruby The Hard Way”. Hopefully with some free time and extra boost of determination I got this last week I should be able to close the gap and finish this book up in the next week or two.\nThis last Wednesday I got the pleasure of attending the Indy.rb meetup group. I can confirm that what people say is true and Ruby does have one of the most open and friendly communities. As well my worries about Ruby jobs in Indy have mostly been quenched, as I learned of several job openings and emailed one of the people there who said he would provide a list of all the Ruby on Rails employers in the area he knows of who are hiring. Over all its a pretty exciting aspect that I might not be to hard to get a Ruby on Rails job in the area hopefully sometime soon.\nOn the social front I uploaded a bunch of code the other day to GitHub. While none if its in Ruby it some of my key projects from learning to code in languages like C++, Java, JavaScript, C#, and Python. Hopefully they will be getting some Ruby friends soon! I have also started recording videos for a youtube channel that will hopefully launch next month depending on free time for setting things up.\nWhile I got to close this out I would like to say thanks for following along in my journey to becoming an Junior Developer, as well as the first official post on my new blog “Program Practical”! More site edits are to come, I just wanted to get it done enough to start releasing content on it as I like to keep up with my every other day postings.\nAs always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer.\n","permalink":"https://blog.oll.is/posts/2016-02-13_mid-month-review/","summary":"It’s been a pretty crazy February for me. However, I wanted to take some time just to catch up on progress and whats going on.\nI’m just passed the 1/2 mark of “Learn Ruby The Hard Way”. Hopefully with some free time and extra boost of determination I got this last week I should be able to close the gap and finish this book up in the next week or two.","title":"Mid Month Review"},{"content":"Working with strings in Ruby has been interesting. As the\nre is far more to pay attention to and what you can do then when dealing with just JavaScript.\nOne of my favorite things is the ability to put a variable right in the middle of a string without having to first break out of it and then add it in.\nJavaScript var my_var = “day”;\nconsole.log(“Isn’t to”+my_var+” just lovely?”);\nRuby my_var = “day”\nputs “Isn’t to#{my_var} just lovely?”\nFor me this is just a more clean and optimal way of writing code, we aren’t breaking out of the string to add a variable and it just looks cleaner. However, you have to be careful as not all quotes are created equal. Double quotes (“) tells Ruby to replace variables it finds with an #{}, but if you used just a single quote (‘) Ruby would leave the string alone and ignore any variables in it.\nmy_var = “day”\nputs “Isn’t to#{my_var} just lovely?”\noutput: Isn’t today just lovely?\u0026gt; my_var = “day”\nputs ‘Isn’t to#{my_var} just lovely?’\noutput: Isn’t to#{my_var} just lovely?\nAs you can see we get quite a different results. Although this could be great for debugging as you can pass over the un-formatted string to the console for review.\nAnother great formatting option in Ruby is using %{} instead of the #{}. With %{} you get the ability to apply the same format to multiple values, which can be great for templementing.\nformatter = “Although its %{temp} I’m really enjoying the %{like}”\nputs formatter % {temp : “cold”, like: “snow”}\nputs formatter % {temp : “raining”, like: “thunder”}\noutput: Although its cold I’m really enjoying the snow\noutput: Although its raining I’m really enjoying the thunder\nThe last example I will show is something I really wish JavaScript could do although I really wish I could do the two things mentioned above in it. Using three double quotation marks in a row indicates a multiline string. While you can do these in JavaScript, Ruby once again just as a cleaner way of doing it as I don’t have to put () after the end of each line.\nJavaScript var my_var = “ This \\\nis \\\nmy \\\nstirng”;\nRuby my_var = “””\nThis\nis\nmy\nstring\n“””\nWhile this might not be groundbreaking I love the cleanness and power Ruby provides to string control in its language. As well I hope if you are just getting into Ruby you have found something here helpful in you ability to control and manipulate strings.\nAs always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer.\n","permalink":"https://blog.oll.is/posts/2016-02-07_string-theory-strings-in-ruby/","summary":"Working with strings in Ruby has been interesting. As the\nre is far more to pay attention to and what you can do then when dealing with just JavaScript.\nOne of my favorite things is the ability to put a variable right in the middle of a string without having to first break out of it and then add it in.\nJavaScript var my_var = “day”;\nconsole.log(“Isn’t to”+my_var+” just lovely?”);","title":"String Theory: Strings in Ruby"},{"content":"Yes, so I am what I would call a hobbiest. I have worked the past 5 years “Learning to program” without any real discipline. Due to this is skipped all of the “challenges” except for the bootstrap ones. However, I still feel their quoted time is horribly off. I have seen a lot of other post from people said they went from 0 to done in under 1000 hours. way under what they quote.\n","permalink":"https://blog.oll.is/posts/2016-02-06_yes-so-i-am-what-i-would-call-a-hobbiest./","summary":"Yes, so I am what I would call a hobbiest. I have worked the past 5 years “Learning to program” without any real discipline. Due to this is skipped all of the “challenges” except for the bootstrap ones. However, I still feel their quoted time is horribly off. I have seen a lot of other post from people said they went from 0 to done in under 1000 hours. way under what they quote.","title":"Yes, so I am what I would call a hobbiest."},{"content":"I have started working on a very small side project while I am learning Ruby. It’s just a simple time tracker that helps me with my current IT admin job. I have been trying some different time trackers like Toggle. While I find them awesome, in fact I’m using Toggle right now to track my time writing, they are just not suited for fast paced, high risk world of IT. Okay, maybe just fast paced sometimes, with no risk. Mainly I just needed something that could track time, I could give it a title, and then a few different text fields. One for private notes, and one for notes that would be up in the labor report.\nThanks to my time at Free Code Camp, not only did I have the knowledge that I could accomplish this, but I could even estimate how long parts of the project would take. During the creation of this application I am also studying Ruby with Chris Pine’s Learn To Program. One of the odd things about ruby is how it handles integers in strings.\nI am very used to JavaScript where almost anything goes when putting things into strings. However, in Ruby you have to tell it “Hello, Player “ + my_number.to_s. This is especially true when adding single characters that are numbers to integers. Chris points out that by doing this how is the computer suppose to know what you want? If you had 2 + ‘5’, are you asking for 25 or 7?\nIn my program you can have multiple timers running at any given time. To simply running them all, I don’t store a list of all the timers time, I just get time time off the timer every second, update, and display the new time. I was doing something along the lines of the code below.\n//Get time stamp, and split into minutes and seconds\nvar timer = ($(event).html()).split(“:”);\n//Add turn time into seconds and add 1\ntimer = (timer[0] * 60) + timer[1] + 1;\n/** Do turn back into string and update display **/\nWhile this is legal JavaScript I was really shocked when after the timer hit a minute I was getting results like, 59, 1:01, 100:02, 10000:03. I was really confused on what was going on here. Luckily I had just read the chapter in learn to program that I mentioned before. The compiler really isn’t sure what I was waning so it just joined the stings as it saw proper. I updated my code with what you see below.\n//Get time stamp, and split into minutes and seconds\nvar timer = ($(event).html()).split(“:”);\n//Add turn time into seconds and add 1\ntimer = Number(timer[0] * 60) + Number(timer[1]) + 1;\n/** Do turn back into string and update display **/\nThis time I specified that the string objects “timer” are to be treated as numbers and not strings. I was shocked to already start seeing some benefits to Ruby, but mostly that even now as someone new to the Ruby language I can see why people push that its a good language to learn. At first I was really thinking to have to call “.to_s” would just be tedious, it’s there for a good reason, and for newer programmers learning JavaScript the error I ran into might not be such an easy one to solve, that would have just been avoided by using a language that is a little more locked down.\nAs always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer.\n","permalink":"https://blog.oll.is/posts/2016-02-05_javascript-lesson-learned-thanks-to-ruby/","summary":"I have started working on a very small side project while I am learning Ruby. It’s just a simple time tracker that helps me with my current IT admin job. I have been trying some different time trackers like Toggle. While I find them awesome, in fact I’m using Toggle right now to track my time writing, they are just not suited for fast paced, high risk world of IT. Okay, maybe just fast paced sometimes, with no risk.","title":"JavaScript Lesson Learned Thanks to Ruby"},{"content":"Great post, I have always been a fan of making apps for niche markets because of this. Sure my audience isn’t as broad but its a safer market.\n","permalink":"https://blog.oll.is/posts/2016-02-03_great-post-i-have-always-been-a-fan-of-making-apps-for-niche-markets-because-of-this./","summary":"Great post, I have always been a fan of making apps for niche markets because of this. Sure my audience isn’t as broad but its a safer market.","title":"Great post, I have always been a fan of making apps for niche markets because of this."},{"content":" With my Front End Developer Certificate behind me I start the next leg of my journey. I am mainly following the guidelines laid out in in Joshua kemp’s book “No Degree No Problem”. As of Monday I officially started working though the book Learning to Program by Chris Pine.\nRight off the bat I’m enjoying Ruby but it is very odd programming in a language without curly brackets. As the majority of my time programming as been on in C based languages I find this one a little odd.\nHowever laying out the plans I will spend this month focusing on two things. Getting though Learning to program and Learn Ruby the Hard Way. As well as finishing up my portfolio site. Once done with the books I’ll be moving onto the online course “One Month Rails”. This should take me into April, when I will going back to Free Code Camp! However its mostly to redo the algorithm challenges in the Ruby language. Particularly the Intermediate and Advance challenges. Last real learning from here other than open source projects and freelance work until getting hired will be completing the “Michael Hartl’s Rails Tutorial”.\nIt’ll be a crazy half year, as well during this time I’ll be looking a potential web development jobs, that if landed could throw things off schedule if they have tech they want me to learn but one day at a time. However the overall plan listed in my first post stays the same. Study 21 hours per week. Attend 4–5 meetups a month, although as I’m finding it might be more like 2–4 due to regularity in meetup events. I’ll be posting on here, or possibly my own blog if I decided to run one myself, at least 3 time a week and on twitter daily.\nI’ll also be launching another channel here soon, more to come on that as plans form.\nAs always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer.\n","permalink":"https://blog.oll.is/posts/2016-02-03_step-2-learning-ruby/","summary":"With my Front End Developer Certificate behind me I start the next leg of my journey. I am mainly following the guidelines laid out in in Joshua kemp’s book “No Degree No Problem”. As of Monday I officially started working though the book Learning to Program by Chris Pine.\nRight off the bat I’m enjoying Ruby but it is very odd programming in a language without curly brackets. As the majority of my time programming as been on in C based languages I find this one a little odd.","title":"Step 2: Learning Ruby"},{"content":" Last night I was able to finish up the Simon project wrapping up the last of the requirements for Free Code Camp’s Front End Development Certificate. I am pretty excited to have completed this, while it’s just a certificate from a free online site, it means the completion of the first step in my journey.\nIt only took me about 3 weeks in all to get through all the required algorithms and projects. I really enjoyed the algorithms, they were challenging, and got me to finally understand Regular Expression. The projects also had a lot of fun in them although I only see 4 projects in all that I would call complex or do enough to be worth putting on a portfolio.\nFour Function Calculator. Good basic project for showing JavaScript control and updating DOM and using Event Handlers Wikipedia Search. Example of using public API’s from a major service. Simple project but gets the point across. Tic-Tac-Toe. Very simple project UI wise, but shows basic AI ability though JavaScript that is just fun to make Simon. Last and best project in my opinion. Not only does it take a ton (compared to projects prior) to make look like a simon game, but the JavaScript logic needed and heavy use of timers makes it a real challenge. Looking forward to my next steps. Many people have gone, on after getting their Front End Developer Certification, and gotten jobs working as web developers. However, I’m not sure this is the path I am wanting to go down. I have a goal of being a Junior Developer, particularly I would love to get into Backend or Full Stack development. I worry I wouldn’t find enough fulfillment in just doing front end web design, but this is coming from someone who has only done it as a hobby from time to time. Although if it was for a tech company or a company whose first priority is web I would be interested. My main fear would be to work for a company whose website is more of just an afterthought, and with that comes the lower salary, and lower respect for what you do.\nToday my books on Ruby should be arriving in the mail and I will being a month long frantic study session to get though both of them before March 1st. I will post more about the books soon, as well as some possible videos based on getting started with Ruby on Rails.\nAs always thanks for reading, and follow me on twitter for more daily updates and post on my journey from Senior IT Admin to Junior Developer.\n","permalink":"https://blog.oll.is/posts/2016-02-01_certified-front-end-developer/","summary":"Last night I was able to finish up the Simon project wrapping up the last of the requirements for Free Code Camp’s Front End Development Certificate. I am pretty excited to have completed this, while it’s just a certificate from a free online site, it means the completion of the first step in my journey.\nIt only took me about 3 weeks in all to get through all the required algorithms and projects.","title":"Certified Front End Developer"},{"content":"I have been working both extensively on my coding, and trying to come up with the “next steps” after I get done with Free Code Camp’s Front End Developer certification. Something I should hopefully have done in the next several days. Looking forward I come back to a question I asked in an earlier post about picking Ruby or JavaScript for continued learning for back-end/full stack development.\nThere has been a lot of great discussions with developers and local groups about the options. I have experience with both languages, although it’s more of just getting my feet wet with Ruby about a year ago. Even if I do go with Ruby I can see the importance’s of JavaScript meetups, as a lot of them focus on front end libraries, not just stuff like Node.js.\nI’m actually set to go to the indy.rb meetup group (and the new bloom.rb next time they meet) to meet the local community and to see what it’s all about. The guy who runs the groups has told me about how he just got done mentoring 5 guys who knew very little programming to Ruby on Rails developers because a local company needed developers so badly but can’t find any.\nThis is interesting because, while I see more JavaScript jobs on average, Ruby is reportedly growing in the area. I was stuck in the mind set of, more jobs equals a higher demand for programmers. I didn’t stop to think that the programmer base might be so low, that while jobs are not as plentiful companies are in so much more of a desperate need for the developers. Making entering the job market with only sample projects to serve as experience might be more than enough to land a job, and be an asset right off the bat. I have been listing to the audio book Soft Skills, in the book John talks about specializing to make yourself needed. He started as a programmer that specialized in printer languages, this gave him an amazing advantage in the job market because of the low employee pool of programmers that specialized in programming for printers.\nLooking at the job trends in Indiana for both Ruby and JavaScript on Gooroo’s site I found some interesting things.\nJavaScript [caption id=”attachment_23\u0026quot; align=”aligncenter” width=”610\u0026quot;]\nA good amount of month jobs advertised and pay[/caption]\n[caption id=”attachment_22\u0026quot; align=”aligncenter” width=”569\u0026quot;]\nJob posting is down almost 1/2[/caption]\nWhile one sites job posting might not be the best place to “confirm” overall posting and industry trends it’s still something to note. It’s also shouldn’t be surprising with places like Free Code Camp and so many others teaching people to code in JavaScript the pool of people trying to become a Junior JavaScript Developer is increasing. With this demand is being met and job numbers are coming down, in this can by almost half. However, 178 jobs / month is still a fantastic number if you ask me and makes JavaScript still a great choice.\nRuby [caption id=”attachment_25\u0026quot; align=”aligncenter” width=”561\u0026quot;]\nLow job posting, but higher pay[/caption]\n[caption id=”attachment_24\u0026quot; align=”aligncenter” width=”567\u0026quot;]\nJob posting up by 25%[/caption]\nRuby is a mixed bag, There are only 35 postings / month, although the pay is better, though not by much. Job posting has also gone up by almost 25% over the past year. While it’s still below JavaScript in over all posting by quite a bit it’s still a good prospect that jobs in the field are growing locally. As well if I move my job search to Chicago, as I am willing to move to Chicago for jobs, not only can you find 181 jobs / month in Chicago alone but some Junior Ruby Developer jobs have starting pays just below 100k a year.\nWhile I am still reviewing things as I’m very analytical and always second guess myself. I have to say the road is looking Ruby lined to me. While I’ll never give up on JavaScript as you can do some great things client side with it my overall goal is back-end or full stack development, so I’m more worried about the backbone of application development. I’m sure sometime next week or the week after I will have my mind made up for sure. However, if what the indy.rb attendants are telling me the truth about need, ruby will be the easiest way to find my first development job.\nMore to come, thanks for reading and keep on learning.\n","permalink":"https://blog.oll.is/posts/2016-01-30_road-lined-with-rubies/","summary":"I have been working both extensively on my coding, and trying to come up with the “next steps” after I get done with Free Code Camp’s Front End Developer certification. Something I should hopefully have done in the next several days. Looking forward I come back to a question I asked in an earlier post about picking Ruby or JavaScript for continued learning for back-end/full stack development.\nThere has been a lot of great discussions with developers and local groups about the options.","title":"The Road Lined with Rubies"},{"content":"Open source and example projects was major for my first Android App. I actually got a book called “Android for Programmers: An App-Driven Approach”. This was amazing because I got to see how these apps are made and put together instead or just reading the documentation and not knowing how things go together. Great article.\n","permalink":"https://blog.oll.is/posts/2016-01-29_open-source-and-example-projects-was-major-for-my-first-android-app./","summary":"Open source and example projects was major for my first Android App. I actually got a book called “Android for Programmers: An App-Driven Approach”. This was amazing because I got to see how these apps are made and put together instead or just reading the documentation and not knowing how things go together. Great article.","title":"Open source and example projects was major for my first Android App."},{"content":"Fantastic list, I was just recommended http://www.exercism.io/ from a redditor as a place to find an (almost) never ending list of projects to help further learning to code.\n","permalink":"https://blog.oll.is/posts/2016-01-29_fantastic-list-i-was-just-recommended-httpwww.exercism.io/","summary":"Fantastic list, I was just recommended http://www.exercism.io/ from a redditor as a place to find an (almost) never ending list of projects to help further learning to code.","title":"Fantastic list, I was just recommended http://www.exercism.io/"},{"content":"Yeah I have yet to get into Node.JS. However, the Ruby community in Indy has been amazingly been having some amazing communications with me via Reddit. Plan is to head to their meet up and to see what Ruby is all about. Apparently there is a massive lack of Ruby developers in Indiana I’m not aware of.\n","permalink":"https://blog.oll.is/posts/2016-01-29_yeah-i-have-yet-to-get-into-node.js./","summary":"Yeah I have yet to get into Node.JS. However, the Ruby community in Indy has been amazingly been having some amazing communications with me via Reddit. Plan is to head to their meet up and to see what Ruby is all about. Apparently there is a massive lack of Ruby developers in Indiana I’m not aware of.","title":"Yeah I have yet to get into Node.JS."},{"content":"I have been pretty happy with Free Code Camp. They make a pretty easy approach, and if you are not new you don’t have to sit through all the “challenges” just go straight to the Algorithms and Projects to get Certified. To date I have fully completed:\nResponsive Design with Bootstrap Basic Algorithms Basic Projects Intermediate Algorithms I am currently working on the “Intermediate Projects” about 2/3 though with only 2 projects left. Although the basic projects were basic, they were fun and usually pretty different. From making a 4 function calculator, portfolio, pomodoro timer, to a random quote generator they each are pretty different. After doing the Intermediate Algorithms, I was pretty excited to get into the projects as they would be my last requirement from getting the Front End Developer Certification from Free Code Camp.\nIn this collection of Projects you have to complete a total of 6. I have just finished the 4th one and at this point pretty frustrated with the course materials. The first four projects all just have to deal with JSON API’s. The first one you make a weather app. Great you get to play with your first API, although they tell you no where you can’t use geo location due to CodePen limitations. This is frustrating because they don’t teach you how to use the javascript console yet, and the error is only in there. As well they teach your geo location out of nowhere so it’s safe to assume they mean for you to use it.\nThe next 3 projects are all the same, this time you get an array of data back from your call and you need to organize that data into a list/grid. Timing my development times the projects each take about 2 hours especially because as you get to the 3rd and 4th project to have all the logic memorized and aren’t doing anything more than wasting your time due to not being challenged. If I wasn’t so close to being done I would probably have stopped.\nHowever, there is light at the end of the tunnel. The last two projects is a Tic-Tac-Toe game with AI, and Simon Says. These are the real projects I was expecting in an intermediate collection. I wouldn’t have mind at all if they did 1 or even 2 of those API projects but to do 4 and then get to some projects that will actually require some thought makes it seem like they couldn’t come up with anything they deemed as “intermediate” and decided to teach you how to reskin a projects.\nHonestly with the reviews I have seen of the Backend section, and the experience from this last section I’m not sure if Free Code Camp is the place to learn Node.js if I do stay with JS and not switch over to Ruby on Rails. I love the Free Code Camp community and they admins do listen. So hopefully in time they will get some better projects. It’s just sad to me that someone who might have gone through the entire program and all the challenges would get such basic projects to finish out their first stage of learning. I truly feel I have become a much better programmer with their algorithm challenges and even still love to show off a few of the projects I have made. I can thank Free Code Camp for that. I don’t mean to come this off as a bashing article. It’s just something I have been frustrated with as I have spent all week on these projects.\n","permalink":"https://blog.oll.is/posts/2016-01-28_let-down-near-the-finish-line/","summary":"I have been pretty happy with Free Code Camp. They make a pretty easy approach, and if you are not new you don’t have to sit through all the “challenges” just go straight to the Algorithms and Projects to get Certified. To date I have fully completed:\nResponsive Design with Bootstrap Basic Algorithms Basic Projects Intermediate Algorithms I am currently working on the “Intermediate Projects” about 2/3 though with only 2 projects left.","title":"The Let Down Near the Finish Line"},{"content":"I have been struggling a lot lately in trying to decide my next steps as I finish Free Code Camp’s Front End Developer Certification. I figured I would blow through it quickly to get my legs back into web development after spending a good amount of time in C++ trying to build a 2d game engine.\nWhile it has been fun and I have got a friend started on programming using FCC’s easy to learn curriculum finding a job still has me perplexed. After reading/listening to Joshua kemp’s “No Degree, No Problem” I have been doing a lot of review in Ruby on Rails. I find many articles out there about how good the community is and how it's “exploding but has a lack of developers” making it easy for find Junior Developer jobs in the field. I see a lot of people refer to the image below from indeed showing the rate of growth compared to javascript.\nI left Node.JS out of the first one so we could see the other 3 well enough. As you can see below once we add in Node.JS the other ones are barely on the map. Now this could be attributed to Node.JS being new and the fact it come from zero to Ruby on Rails level in terms of % of job market posting in just 2–3 years. However, it brings up the question. Is Node.JS better to learn over Ruby on Rails for Junior Developers?\nThis isn’t helped when you look at percentage of matching job positions. Javascript is not only much higher, but Node.JS has passed Ruby on Rails.\nWhile I love to see so much growth in Ruby on Rails, it seems it would still be easier to get a javascript based job. Although it’s important to remember this is all meta data so this doesn’t distinguish Junior/Mid/Senior level jobs and is more of just a keyword search than anything.\nHowever, it stands what are the jobs in my area (Bloomington, IN, or anywhere in Indiana) while there seems to be a good amount of web development jobs none really talk about backend. A quick look on indeed.com shows there are some Junior Ruby Developer positions in Chicago although that would require an out of state relocation, something I’m not apposed to, just takes a lot more planning than a 50 mile commute. Of course Josh Kemp brings up the importance that jobs are found via networking not job posting. This is true for my current job, as it's not advertised it was just knowing the right person, same with my friends job in a data center. Meetup.com have tons of both Javascript and Ruby on Rails meet ups in the near 75 miles that I searched, meaning their could be a good Ruby job base in the state I’m not able to find by searching.\nSo what's better? I’m honestly not sure. Of course there could be many “it depends”. I will continue to research positions and trying to talk to the few developers I know. I find many more sites online that point out Ruby on Rails is the best for full stack, and Node.JS is more of an emerging market, but still uncertain. If you have any thoughts to add, or articles that I might have missed please feel free to post them as I would love some more insight as I prepare to finish up the current leg of my journey and prepare for the sprint to hopefully the finish line.\n","permalink":"https://blog.oll.is/posts/2016-01-26_ruby-over-javascript/","summary":"I have been struggling a lot lately in trying to decide my next steps as I finish Free Code Camp’s Front End Developer Certification. I figured I would blow through it quickly to get my legs back into web development after spending a good amount of time in C++ trying to build a 2d game engine.\nWhile it has been fun and I have got a friend started on programming using FCC’s easy to learn curriculum finding a job still has me perplexed.","title":"Ruby over JavaScript?"},{"content":"Thanks for the advice, I’m lucky enough to always find time to code daily, I have found locking myself down to 21 hours of coding a week has really accelerated my understanding and speed. I will look into DevOps automation. I had not be given that advice before, but it makes a lot of sense.\n","permalink":"https://blog.oll.is/posts/2016-01-25_thanks-for-the-advice-im-lucky-enough-to-always-find-time-to-code-daily-i-have-found-locking/","summary":"Thanks for the advice, I’m lucky enough to always find time to code daily, I have found locking myself down to 21 hours of coding a week has really accelerated my understanding and speed. I will look into DevOps automation. I had not be given that advice before, but it makes a lot of sense.","title":"Thanks for the advice, I’m lucky enough to always find time to code daily, I have found locking…"},{"content":"One of the things I try to push on my friends learning code is don’t get stuck on a lesson for 5 hours that should have taken 30 minutes just because you don’t fully understand it. Keep working it will come up enough that sooner or later the light bulb will turn on.\n","permalink":"https://blog.oll.is/posts/2016-01-25_one-of-the-things-i-try-to-push-on-my-friends-learning-code-is-dont-get-stuck-on-a-lesson-for-5/","summary":"One of the things I try to push on my friends learning code is don’t get stuck on a lesson for 5 hours that should have taken 30 minutes just because you don’t fully understand it. Keep working it will come up enough that sooner or later the light bulb will turn on.","title":"One of the things I try to push on my friends learning code is don’t get stuck on a lesson for 5…"},{"content":"One of the big reasons I love Free Code Camp is the simplicity behind it, and how advanced it gets when you get into projects. While I was sad to see the terminology go, I was happy to see how quickly changes were made when the community saw the new site in put in input on how to improve. I hear Ruby has the best community out there of any language. However, I wonder if that because they haven’t stumbled on the community at Free Code Camp.\n","permalink":"https://blog.oll.is/posts/2016-01-25_one-of-the-big-reasons-i-love-free-code-camp-is-the-simplicity-behind-it-and-how-advanced-it-gets/","summary":"One of the big reasons I love Free Code Camp is the simplicity behind it, and how advanced it gets when you get into projects. While I was sad to see the terminology go, I was happy to see how quickly changes were made when the community saw the new site in put in input on how to improve. I hear Ruby has the best community out there of any language.","title":"One of the big reasons I love Free Code Camp is the simplicity behind it, and how advanced it gets…"},{"content":"We all start somewhere I suppose. I’ve been “learning” to code for over 5 years now. Yet, here I am a Senior IT Administrator, not a programmer with very few projects to show of my years or “learning”. Its easy not to stay focus on one project or another, or wanting to jump to the newest language. I made a promise to myself, that this would be the year I hunker down, study, and apply for jobs as a Junior Developer.\nI’m hoping to track my journey well so that others that are attempting to self teach themselves might not lose 5 years like I did. As well hoping just to have public accountability for studying and staying on schedule.\nWhile I plan on posting soon the full plan and how I got on this road I will start tonight with posting a brief overview.\nI plan on studying coding for 21 hours a week. Broken down into about 2–3 hours days M-F, ~6–8h on Sat, and what ever is left on Sun. I also plan on posting 2–3 Medium post a week with hoping 3–4 tweets / day detailing and following my journey. There is also a goal for going to 4–5 coding meetup/events a month. Unfortunately I live in Bloomington and the groups are limited, so a 50–70 mile drive is needed for most events. Although I’m working hard to try and work 1 trip a week into my schedule.\nGot to start somewhere.\n","permalink":"https://blog.oll.is/posts/2016-01-24_staring-line/","summary":"We all start somewhere I suppose. I’ve been “learning” to code for over 5 years now. Yet, here I am a Senior IT Administrator, not a programmer with very few projects to show of my years or “learning”. Its easy not to stay focus on one project or another, or wanting to jump to the newest language. I made a promise to myself, that this would be the year I hunker down, study, and apply for jobs as a Junior Developer.","title":"Staring Line"}]